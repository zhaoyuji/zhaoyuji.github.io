<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Z's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 07 Nov 2019 11:50:00 +0100</pubDate>
    <lastBuildDate>Thu, 07 Nov 2019 11:50:00 +0100</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>[MLAPP] Chapter 3: Generative Models for Discrete Data</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [ ['$$', '$$']],
            inlineMath: [['$','$']],
            processEscapes: true
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;31-introduction&quot;&gt;3.1 Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Generative Models aim to model $P(X,y)$&lt;/li&gt;
  &lt;li&gt;While discriminative Models aim to directly model $P(y|X)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Applying Bayes rule to a generative classifier of the form:
&lt;script type=&quot;math/tex&quot;&gt;p(y = c|x, \theta) \propto p(x|y = c, \theta) \cdot p(y = c|\theta)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;32-bayesian-concept-learning&quot;&gt;3.2 Bayesian concept learning&lt;/h2&gt;

&lt;p&gt;In reality, it is considered that children learn from positive examples and obtain negative examples during an active learning process, such as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parents: Look at the cute dog! &lt;br /&gt;
Children(Point out a cat): Cute Doggy!&lt;br /&gt;
Parents: Thatâ€™s a cat, dear, not a dog&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt; psychological research has shown that people can learn concepts from positive examples alone (Xu and Tenenbaum 2007).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Concept learning: We can think of learning the meaning of a word as equivalent to concept learning, which in turn is equivalent to binary classification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A example on number game to introduce: &lt;em&gt;posterior predictive distribution, induction, generalization gradient, hypothesis space, version space.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;æ³›åŒ–æ¢¯åº¦ï¼ˆgeneralization gradientï¼‰æ˜¯æŒ‡ç›¸ä¼¼æ€§ç¨‹åº¦ä¸åŒçš„åˆºæ¿€å¼•èµ·çš„ä¸åŒå¼ºåº¦çš„ååº”çš„ä¸€ç§ç›´è§‚è¡¨å¾ã€‚å®ƒè¡¨æ˜ äº†æ³›åŒ–çš„æ°´å¹³ï¼Œæ˜¯æ³›åŒ–ååº”å¼ºåº¦å˜åŒ–çš„æŒ‡æ ‡ã€‚&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;321-likelihood&quot;&gt;3.2.1 Likelihood&lt;/h4&gt;

&lt;p&gt;è¿™ä¸ªä¾‹å­å¾ˆæœ‰è¶£ï¼å½“ä½ çœ‹åˆ°$D={16}$çš„æ—¶å€™ï¼Œä½ ä¼šè§‰å¾—å®ƒå±äºå“ªä¸ªæ•°æ®é›†ï¼ˆeven or power of 2 ?ï¼‰ï¼Œä½†å½“ä½ çœ‹åˆ°$D={16, 2, 8, 64}$çš„æ—¶å€™å‘¢ï¼Ÿå®éªŒè¯æ˜äººä»¬ä¼šå€¾å‘äºé€‰æ‹©è®¤ä¸ºä»–ä»¬æ˜¯power of 2è¿™ä¸ªæ•°æ®é›†ã€‚&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The key intuition is that we want to avoid &lt;strong&gt;suspicious coincidences&lt;/strong&gt;. If the true concept was even numbers, how come we only saw numbers that happened to be powers of two?&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;extension&lt;/strong&gt; of a concept is just the set of numbers that belong to it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Strong sampling assumption&lt;/strong&gt;: we assume that our data points are drawn uniformly and independently. Given this assumption, the probability of independently sampling N items (with replacement) from h is given by&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(D|h)=\Big[\frac{1}{size(h)}\Big]^N=\Big[\frac{1}{|h|}\Big]^N&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Size principle&lt;/strong&gt; the model favors the simplest (smallest) hypothesis consistent with the data. This is more commonly known as &lt;strong&gt;Occamâ€™s razor&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;å¥¥å¡å§†å‰ƒåˆ€å®šå¾‹ï¼ˆOccamâ€™s Razor, Ockhamâ€™s Razorï¼‰åˆç§°â€œå¥¥åº·çš„å‰ƒåˆ€â€ï¼Œå®ƒæ˜¯ç”±14ä¸–çºªè‹±æ ¼å…°çš„é€»è¾‘å­¦å®¶ã€åœ£æ–¹æµå„ä¼šä¿®å£«å¥¥å¡å§†çš„å¨å»‰ï¼ˆWilliam of Occamï¼Œçº¦1285å¹´è‡³1349å¹´ï¼‰æå‡ºã€‚è¿™ä¸ªåŸç†ç§°ä¸ºâ€œå¦‚æ— å¿…è¦ï¼Œå‹¿å¢å®ä½“â€ï¼Œå³â€œç®€å•æœ‰æ•ˆåŸç†â€ã€‚æ­£å¦‚ä»–åœ¨ã€Šç®´è¨€ä¹¦æ³¨ã€‹2å·15é¢˜è¯´â€œåˆ‡å‹¿æµªè´¹è¾ƒå¤šä¸œè¥¿å»åšï¼Œç”¨è¾ƒå°‘çš„ä¸œè¥¿ï¼ŒåŒæ ·å¯ä»¥åšå¥½çš„äº‹æƒ…ã€‚&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;å¯¹äºä¸Šé¢è¿™ä¸ªä¾‹å­ï¼Œæ¯”å¦‚ï¼Œ100ä»¥ä¸‹çš„2çš„å€æ•°çš„æ•°å­—åªæœ‰6ä¸ªï¼Œä½†æ˜¯æ˜¯å¶æ•°çš„æœ‰50ä¸ªï¼Œé‚£ä¹ˆ&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(D|h_{two})=1/6,p(D|h_{even})=1/50&lt;/script&gt;

&lt;p&gt;ç”±äºå‡ºç°äº†4ä¸ªä¾‹å­ï¼ˆ{16, 2, 8, 64}ï¼‰ï¼Œé‚£ä¹ˆ$h_{two}$çš„likelihoodæ˜¯$(1/6)^4$ï¼Œ$h_{even}$çš„likelihoodæ˜¯$(1/50)^4$ï¼Œ &lt;strong&gt;likelihood ratio&lt;/strong&gt;å‡ ä¹æ˜¯5000:1ï¼Œæ‰€ä»¥è¯´æˆ‘ä»¬å€¾å‘äºè®¤ä¸ºè¿™ç»„æ•°æ®å‡ºè‡ªpower of 2çš„æ•°æ®é›†ã€‚&lt;/p&gt;

&lt;h4 id=&quot;322-prior&quot;&gt;3.2.2 Prior&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Bayesian reasoning is &lt;strong&gt;subjective&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Different People will have different priors, also different hypothesis spaces.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;323-posterior&quot;&gt;3.2.3 Posterior&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;The posterior is simply the likelihood times the prior, normalized.
  &lt;script type=&quot;math/tex&quot;&gt;p(h|D)=\frac{p(D|h)p(h)}{\sum_{h'\in \mathcal{H}}p(D,h')}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;In the case of most of the concepts, the prior is uniform, so the posterior is proportional to the likelihood.&lt;/li&gt;
  &lt;li&gt;â€œUnnaturalâ€ concepts of â€œpowers of 2, plus 37â€ and â€œpowers of 2, except 32â€ have low posterior support, despite having high likelihood, due to the low prior.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MAP estimate:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{h}^{MAP}=argmax_h \ p(D|h)p(h)=argmax_h \ \log p(D|h)+ \log p(h)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As we get more and more data, the MAP estimate converges towards the maximum likelihood estimate or MLE:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\widehat{h}^{mle}=argmax_h \ p(D|h)=argmax_h \ \log p(D|h)&lt;/script&gt;

    &lt;p&gt;In other words, if we have enough data, we see that &lt;strong&gt;the data overwhelms the prior&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;324-posterior-predictive-distribution&quot;&gt;3.2.4 Posterior predictive distribution&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;The posterior is our internal belief state about the world.&lt;/li&gt;
  &lt;li&gt;We should justify them by predicting
  &lt;script type=&quot;math/tex&quot;&gt;p(\tilde{x} \in C|D)=\sum_h p(y=1|\tilde{x},h)p(h|D)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;This is just a weighted average of the predictions of each individual hypothesis and is called &lt;strong&gt;Bayes model averaging&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;?need to be read?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;325-a-more-complex-prior&quot;&gt;3.2.5 A more complex prior&lt;/h4&gt;

&lt;h2 id=&quot;33-the-beta-binomial-model&quot;&gt;3.3 The beta-binomial model&lt;/h2&gt;

&lt;h4 id=&quot;331-likelihood&quot;&gt;3.3.1 Likelihood&lt;/h4&gt;
&lt;p&gt;Suppose $X_i \sim Ber(\theta) $, and $X_i = 1$ represents heads while $X_i = 0$ represents tails. $\theta \in [0, 1]$ is the rate parameter of probability of heads. If the data are iid, the likelihood has the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(D|theta)=\theta^{N_1}(1-\theta)^{N_0}&lt;/script&gt;

&lt;p&gt;Now suppose the data consists of the count of the number of heads $N_1$ observed in a fixed number $N$, then $N_1 \sim Bin(N,\theta)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Bin(k|n,\theta) = \binom{n}{k}\theta^{n}(1-\theta)^{n-k}&lt;/script&gt;

&lt;h4 id=&quot;332-prior&quot;&gt;3.3.2 Prior&lt;/h4&gt;
&lt;p&gt;To make the math easier, it would convenient if the prior had the same form as the likelihood,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta)\propto \theta^{\gamma_1}(1-\theta)^{\gamma_2}&lt;/script&gt;

&lt;p&gt;for some prior parameters $\gamma_1$ and $\gamma_2$.&lt;/p&gt;

&lt;p&gt;Then the posterior is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta) \propto p(D|\theta)p(\theta)= \theta^{N_1+\gamma_1}(1-\theta)^{N_0+\gamma_2}&lt;/script&gt;

&lt;p&gt;When the prior and the posterior have the same form, we say that the prior is a &lt;strong&gt;conjugate prior&lt;/strong&gt; for the corresponding likelihood.&lt;/p&gt;

&lt;p&gt;In the case of the Bernoulli, the conjugate prior is the beta distribution,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Beta(\theta|a,b) \propto \theta^{a-1}(1-\theta)^{b-1}&lt;/script&gt;

&lt;p&gt;The parameters of the prior are called &lt;strong&gt;hyper-parameters&lt;/strong&gt;. (we set them!)&lt;/p&gt;

&lt;h4 id=&quot;333-posterior&quot;&gt;3.3.3 Posterior&lt;/h4&gt;
&lt;p&gt;If we multiply the likelihood by the beta prior we get the following posterior&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta|D) \propto p(D|\theta)p(\theta) = Bin(N_1|\theta, N_0+N_1)Beta(\theta|a,b)Beta(\theta|N_1+a,N_0+b)&lt;/script&gt;

&lt;p&gt;batch mode v.s. sequential mode??&lt;/p&gt;

</description>
        <pubDate>Mon, 26 Aug 2019 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2019/08/26/MLAPP-3Generativemodels/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/26/MLAPP-3Generativemodels/</guid>
        
        <category>Machine Learning</category>
        
        <category>Learning Notes</category>
        
        
      </item>
    
      <item>
        <title>Striking Moments</title>
        <description>&lt;hr /&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;ç”¨ä»¥è®°ä¸‹ç°åœ¨æ…¢æ‚ æ‚ &lt;strong&gt;ã€Œç”Ÿæ´»ã€&lt;/strong&gt;ä¸­é›¶é›¶ç¢ç¢çš„ç‰‡åˆ»æ„Ÿå—ï¼Œ&lt;strong&gt;è·³è·ƒ&lt;/strong&gt;å¼æµæ°´è´¦è€Œå·²&lt;/li&gt;
  &lt;li&gt;æœ€å¼€å§‹å–åå«åšã€ŒMy Momentsã€ï¼Œè§‰å¾—æœ‰äº›çŸ«æƒ…ï¼Œåˆæ”¹å›äº†ã€Œ&lt;strong&gt;Moments&lt;/strong&gt;ã€&lt;/li&gt;
  &lt;li&gt;å¿½ç„¶&lt;strong&gt;striking&lt;/strong&gt;è¿™ä¸ªè¯å°±è¹¦å‡ºæˆ‘çš„è„‘æµ·ï¼Œè§‰å¾—ååˆ†å¥‘åˆï¼Œå¹¶ä¸”ä¸ç”±è®©æˆ‘å›æƒ³èµ·æ‰“å’çƒçš„é‚£å‡ å¹´ï¼Œå¬åˆ°strikeåå¿ƒæ€é€æ¸å˜å¾—ä¸åŒå´åˆåœ¨æœ€åä¸€åœºæ¯”èµ›è¢«ä¸‰æŒ¯å‡ºå±€åå´©åçš„åœºæ™¯ï¼Œä¹Ÿå¥½åƒå¾ˆé€‚åˆç°åœ¨çš„æˆ‘ã€‚Calm Down My Teammate!&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;10new-angle&quot;&gt;ã€Œ10ã€New Angle&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2019-10-27&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;åœ¨ç»“æŸæ•´æ•´ä¸¤å¤©çš„business challengeçš„æ—¶å€™å³ä½¿æœ€åçš„solutionæ²¡æœ‰é‚£ä¹ˆæ»¡æ„ä½†è¿˜æ˜¯æœ‰ä¸€ç§å¦‚é‡Šé‡è´Ÿxueå¾®è‡ªè±ªçš„æ„Ÿè§‰ï¼Œå¤§çº¦å°±æ˜¯å¦ˆå¦ˆçœ‹å‚»å„¿å­æ€ä¹ˆçœ‹éƒ½å¼€å¿ƒçš„æ„æ€å“ˆå“ˆå“ˆå“ˆã€‚ã€‚ä»æœ€å¼€å§‹çš„æŠµè§¦è¿™ç§å¤´è„‘é£æš´å¼çš„ç«èµ›åˆ°ä¸­é—´å› ä¸ºç»„é‡Œè´¼å¼ºè¿«ç—‡ä¸”åæ‰§çš„å°å“¥å¤´ç–¼ç”šè‡³æƒ³è¿‡ä¸­é€”å¼€æºœåˆ°æœ€åè·Œè·Œæ’æ’å®Œæˆï¼Œç»“æœå¦‚ä½•å¥½åƒä¹Ÿæ²¡æœ‰é‚£ä¹ˆé‡è¦äº†ï¼Œé‡è¦çš„æ˜¯åœ¨é€”ä¸­æƒ³åˆ°äº†hinå¤šç›¸å…³æˆ–æ— å…³äº‹æƒ…ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;99%çš„è¿™æ ·çš„ç‚¹å­éƒ½ä¸æ˜¯é‚£ä¹ˆæœ‰è¶£ï¼Œä½†ä¸€å®šèƒ½æœ‰é‚£ä¹ˆä¸€ä¸¤ä¸ªè®©è‡ªå·±çœ¼å‰ä¸€äº®çš„ideaæˆ–è€…æ˜¯pitchã€‚tell a story and sell your idea è€Œä¸æ˜¯do presentationsï¼Œèƒ½å¤Ÿåœ¨å°ä¸ŠæŠŠpitchåšçš„é‚£ä¹ˆæœ‰è¶£çš„äººå®åœ¨å¤ªæ£’äº†ï¼ï¼ä¸ºä»€ä¹ˆè‡ªå·±åšä¸åˆ°å•Š555[æ³ª]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ä»¥å‰ä¸€ç›´å› ä¸ºæŠµè§¦å·¥ä½œè€Œå»æƒ³è¯»åšå£«ï¼Œæˆ‘åœ¨åœ¨æ¬§æ´²çš„è¿™ä¸¤ä¸ªæœˆé‡Œç»ˆäºèƒ½å¤ŸæŠŠè¿™ç§é€ƒé¿çš„æ€æƒ³é”¤è¿›åœŸé‡Œäº†ã€‚ä¸€ä¸ªæä¸ºä¼˜ç§€çš„ç‹­ä¹‰ä¸Šçš„entrepreneurï¼ˆåˆ›ä¸šè€…ï¼‰å’Œä¸€ä¸ªå¥½çš„ç ”ç©¶è€…é™¤å»æŠ€æœ¯ä¸Šçš„ç¡¬å®åŠ›å…¶å®å¹¶æ— äºŒè‡´ï¼Œä»–ä»¬éƒ½èƒ½è¢«ç§°ä¸ºentrepreneurï¼šå‘ç°é—®é¢˜ï¼Œæ€è€ƒé—®é¢˜ï¼Œæå‡ºè§£å†³æ–¹æ¡ˆï¼Œè®²ä¸€ä¸ªå®Œæ•´çš„æ•…äº‹æœ€ç»ˆå‘ˆç°å®ƒã€‚ä¸å¾—ä¸æ‰¿è®¤é‚£æ—¶å€™å› ä¸ºå·¥ä½œèƒ½åŠ›ä¸å¤Ÿä¸”åŒæ¶å·¥ä½œçš„æˆ‘å¹¸äºæ²¡å»è¯»åšå£«ï¼Œä¸ç„¶ä¹Ÿä¼šå› ä¸ºæ²¡æœ‰å‘ç°è¿™ç§èƒ½åŠ›é‡è¦æ€§çš„è‡ªå·±æ„Ÿåˆ°æ·±æ·±çš„æŒ«è´¥æ„Ÿå§ã€‚ã€‚æœ¬è´¨ä¸Šæˆ‘å¯¹è‡ªå·±çš„ç°çŠ¶çš„æ”¹å˜çš„æœŸæœ›ä¸åº”è¯¥æ˜¯è¯»åšå£«æˆ–è€…å·¥ä½œï¼Œè€Œæ˜¯è¿™ç§é€»è¾‘æ€è€ƒå’Œè¡¨è¾¾èƒ½åŠ›çš„æé«˜ã€‚ã€‚æˆ‘è¿™ä¸ªä¸€å¿ƒåæ‰§ä¸æ„¿æ„æ€è€ƒçš„è ¢äººå•Š[é¡¶]ä»¥åŠè¿™ä¹Ÿæ˜¯å½“å¹´ä¸ºä»€ä¹ˆè‡ªå·±ç ”ç©¶åšçš„ä¸å¥½çš„åŸå› ï¼Œå› ä¸ºå…³æ³¨ç‚¹æ€»ä¸æ˜¯reasonable è€Œæ˜¯ä¸€äº›æ— æ„ä¹‰çš„æŒ‡æ ‡ï¼Œå¿½ç„¶æŒºæ„Ÿè°¢å´”è€å¸ˆçš„contributionè¿é—®çš„å“ˆå“ˆå“ˆå“ˆ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;å¾ˆå¸Œæœ›è‡ªå·±èƒ½å¤Ÿact as an entrepreneurï¼Œå¹¶ä¸”èƒ½æœ‰ä¸”è´¡çŒ®ä¼˜è´¨çš„é«˜å¯†åº¦çš„è®¨è®ºæ€è€ƒï¼Œä½†ä»‹äºè‡ªå·±åœ¨visionä¸Šçš„å·¨å¤§ç¼ºå¤±ä»¥åŠæ ¼å±€ä¹‹å°æ€»æ˜¯æ— æ³•å®ç°[å¤±æœ›]å¸Œæœ›å¤§ä¼™ä¸€èµ·æ¥é­ç­–æˆ‘å¤šè¯»ä¹¦åˆ«ç©æ‰‹æœºäº†[å¤±æœ›]åƒåœ¾ZYJ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ä»¥åŠå¥½å‡ ä¸ªå‹äººæé†’è¿‡æˆ‘çš„ï¼Œåˆä¸€æ¬¡å‘ç°è‡ªå·±æ˜¯ä¸€ä¸ªâ€œåŠŸåˆ©â€çš„äººï¼Œè¯»ä¹¦å¸Œæœ›èƒ½è®°ä½å¾ˆå¤šä¸œè¥¿ï¼Œæ‰€ä»¥è®°ä¸ä¸‹æ¥å°±ä¸æƒ³çœ‹äº†ï¼›å°ç»„è®¨è®ºå¸Œæœ›èƒ½å¾ˆæœ‰è´¡çŒ®ä¸”è¢«é‡‡çº³ï¼Œè™šè£å¿ƒä½œç¥Ÿçš„è´£ä»»æ„Ÿï¼ˆï¼Ÿå¥‡æ€ªçš„è¡¨è¿°ï¼‰ï¼Œæ— æ³•å®ç°çš„æ—¶å€™å°±å˜å¾—æ¶ˆæï¼›æ€»æ˜¯è¿½æ±‚ç»“æœè‡³ä¸Šï¼›åœ¨ä¸è¯¥å½“â€œåºŸç‰©â€çš„æ—¶å€™ä¸å½“ï¼Œè¯¥å½“â€œåºŸç‰©â€çš„æ—¶å€™çå½“ï¼ˆçŸ­æ—¶é—´å†…å¯èƒ½æ— æ³•è·å–ä¸œè¥¿è§‰å¾—æµªè´¹æ—¶é—´ä½†æ˜¯åœ¨é•¿æ—¶é—´å†…æ˜¯æœ‰å·¨å¤§æ”¶è·çš„ï¼‰ï¼Œæ€»æ˜¯æµ®èºåˆæ€¥åŠŸè¿‘åˆ©[æ„Ÿå†’]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;å› ä¸ºå®³æ€•åšä¸åˆ°å®Œç¾å°±å¹²è„†ä¸åšæ˜¯å› å™åºŸé£Ÿçš„è¡¨ç°ï¼Œåœ¨å¼€å£è¯´è‹±è¯­ï¼Œpitchï¼Œåšbusiness ideaç­‰å¤šä¸ªæ–¹é¢ï¼Œæ€»æ˜¯åœ¨åšå¿ƒç†å»ºè®¾ä½†æ€»æ˜¯æ— æ³•è¿ˆå‡ºå¾ˆå¤šç¬¬ä¸€æ­¥ã€‚åˆ°åº•è¦æ€ä¹ˆconvince myselfå‘¢[ç”Ÿç—…]å’Œå¤§è„‘çš®å±‚æ–—äº‰å…‹æœè‡ªå·±çš„æ¡ä»¶åå°„è¡Œä¸ºå®åœ¨æ˜¯å¤ªéš¾äº†ï¼Œå¿«è¦ä¸¤å¹´äº†è¿˜æ˜¯åœ¨ä¸€ä¸ªåœˆå­é‡Œï¼Œæ‰¾åˆ°æ­£åé¦ˆå¤ªéš¾äº†[å¤±æœ›]å¯èƒ½æˆ‘å¤©ç”Ÿå°±æ˜¯åºŸæŸ´å§555555&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æœ€åä¸€ä¸ªå¿½ç„¶æƒ³åˆ°çš„ä¸é‡è¦çš„ç‚¹ï¼Œä»åœ£é©¬ä¸å’Œå¸•æ£®æ–¯çš„è®¾è®¡é£æ ¼ä¹Ÿèƒ½ä»¥å°è§å¤§çœ‹å‡ºæ¥æ¬§æ´²å’Œç¾å›½çš„åŒºåˆ«ï¼Œå°±åƒæ˜¯æ¬§ç¾ç ”ç©¶æ°›å›´çš„å¾ˆå¤§ä¸åŒï¼Œå¤©é©¬è¡Œç©ºé¢ è¦†åˆ›æ–°å’Œå•†ä¸šå®ç”¨ç»“æ„æŠ€èƒ½ã€‚æ‰€ä»¥å¤§å¤šæ•°åœ£é©¬ä¸çš„æ¯•ä¸šç”Ÿéƒ½ç©·å›°æ½¦å€’ï¼Œèƒ½å‡ºä¸€ä¸ªMcQueenå·²ç»æ˜¯å¾ˆä¸å®¹æ˜“æƒ¹ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-10-7&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Scientific Writingè¯¾ä¸Šåˆé™·å…¥å†™research proposalçš„å›°å¢ƒï¼Œå­¦åˆ°ä¸€å¥æ–°çš„è¯ï¼ˆæˆ–è®¸å¯¹å¤§å®¶æ¥è¯´ä¸ç®—å“ˆå“ˆå“ˆå“ˆï¼‰è¿™é‡Œåªæ˜¯å•çº¯è§‰å¾—new angleè¿™ä¸ªè¯å¾ˆé€‚åˆå“ˆå“ˆå“ˆ&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;We solve the problem from a new angle by hypothesizing thatâ€¦..&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;10æœˆçš„ç¬¬ä¸€å‘¨èµ¶äº†å‡ ä¸ªdue&lt;/li&gt;
  &lt;li&gt;å¾ˆä¹…æ²¡å†™æ—¥è®°ï¼Œå°±æœ‰ä¸€ç§è‡ªå·±ä»€ä¹ˆä¹Ÿæ²¡è·å¾—åˆ°çš„æ„Ÿè§‰ï¼ˆå®é™…ä¸Šå¥½åƒä¹Ÿæ˜¯ï¼‰&lt;/li&gt;
  &lt;li&gt;å•Šï¼å¯¹äº†ï¼æœ€è¿‘æœ€å¼€å¿ƒçš„æ˜¯çœ‹äº†ç”µå½±ã€Šç‡•å°¾è¶ã€‹ï¼å¤ªå¥½çœ‹äº†å§ï¼å¥³ä¸»æœ‰äº›åƒæå…°è¿ªï¼å¾ˆé»‘è‰²å¹½é»˜çš„ä¸€éƒ¨ç”µå½±ï¼Œå……æ–¥ç€æš´åŠ›ç¾å­¦ï¼Œä½†ç”µå½±çš„ä¸»é¢˜ä¸Šæœ¬è´¨è¿˜æ˜¯åŠå…¶å°æ¸…æ–°çš„ï¼ˆæ¯•ç«Ÿå¯¼æ¼”å¯æ˜¯å²©äº•ä¿ŠäºŒå•Šå“ˆå“ˆï¼‰éšè—åœ¨è„ã€ä¹±ã€å·®çš„åä¹Œæ‰˜é‚¦çš„å¤–è¡¨ä¸‹çš„å†…æ ¸ä¾æ—§æ˜¯ä¹Œæ‰˜é‚¦å¼çš„ç†æƒ³ä¸»ä¹‰ã€‚å¯¹æ¢¦æƒ³å’Œçˆ±æƒ…æ‰§ç€è¿½æ±‚ç”Ÿå‘½çš„æœ€åä¸€å¤œä¾æ—§æ•´æ™šå”±ç€MAY WAYçš„è‚–é£é¸¿ï¼›æ·±æƒ…æ¸©æŸ”å–„è‰¯ç”šè‡³å¯ä»¥è¯´æœ‰ç‚¹å•çº¯çš„å›ºåŠ›æœï¼›æœæ•¢ä¹‰æ°”ã€è¶…ç„¶ä¸–å¤–çš„èŒä¸šæ€æ‰‹æœ—ï¼›ä»¥åŠä¸€ç›´ä»¿ä½›ç½®èº«äº‹å¤–å±€å¤–äººèˆ¬çš„å‡¤è¶ï¼Œæ— æ—¶æ— åˆ»ä¸åœ¨æ•£å‘ç€æ¸©æƒ…ä¸çˆ±ï¼Œä½†æ‰€æœ‰æƒ…æ„Ÿçš„æµåŠ¨éƒ½å«è“„ä¸”éšç§˜ã€‚&lt;/li&gt;
  &lt;li&gt;ä»¥åŠä»Šå¤©å› ä¸ºä¹‹åæœ‰æœºä¼šinterviewä¸€ä¸ªå¤§äººç‰©è€Œå¼€å¿ƒï¼ï¼ï¼ï¼ï¼ï¼èµï¼ï¼ï¼&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;9back-to-school&quot;&gt;ã€Œ9ã€Back to School&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2019-9-2&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ä¹Ÿæ˜¯ä¸€ä¸ªæ²¡èƒ½èµ·åºŠæ™¨è·‘çš„æ—©ä¸Šï¼Œä¸è¿‡ä¿®å¥½äº†å®¶é‡Œçš„çª—æˆ·ï¼Œç»ˆäºå¯ä»¥é€æ°”äº†ï¼åƒä¸€æ¡è¢«æµ·æµªå†²åˆ°äº†æ²™æ»©ä¸Šçš„é±¼åˆå›åˆ°æµ·æ´‹é‡Œï¼Œç–¯ç‹‚çš„å¸å–å†·ç©ºæ°”ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-3&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;å†èå…¥&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-4&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;äººå°±æ˜¯è¿™æ ·å•Šï¼Œè¯´ç€åªåšä¸¤ä»¶äº‹ä½†æ˜¯ä¸ç”±çš„åˆæƒ³è¦ç»™è‡ªå·±å®‰æ’ä¸Šæ»¡æ»¡çš„è®¡åˆ’ï¼Œç„¶ååˆé™·å…¥ä¸€ä»¶äº‹æƒ…éƒ½åšä¸å¥½çš„å¢ƒåœ°ã€‚åœ¨æ— æ‰€äº‹äº‹çš„é—²æ•£å’Œå¿™ç¢Œç„¦è™‘ä¹‹é—´ä¸åœæ¸¸èµ°ï¼Œæ— æ³•æ‰¾åˆ°å¹³è¡¡ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-5&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To my friendï¼šå¦‚æœèƒ½æ‰¾åˆ°çœŸæ­£å–œæ¬¢çš„äº‹æƒ…é‚£å°±æ”¾å¼€æ‰‹å»åšå§ï¼æ¯•ç«Ÿè¿™ä¸ªå‰æéƒ½å·²ç»æ˜¯å¤§å¤šæ•°äººä¸å…·å¤‡çš„äº†ï¼&lt;/li&gt;
  &lt;li&gt;Be positive!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-6&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ä»Šæ—¥ä»½æ™¨è·‘ + æ‹å°è§†é¢‘ä»»åŠ¡&lt;/p&gt;

    &lt;p&gt;æ–¯äº¬çš„å¤ªé˜³ä¸€å¦‚æ—¢å¾€çš„ç¾ï¼å¥½å¥½ç§¯è“„èƒ½é‡åº¦è¿‡å†¬å¤©å§ï¼&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-09-06-p1.jpeg&quot; alt=&quot;img&quot; /&gt;
 &lt;img src=&quot;/img/in-post/2019-09-06-p2.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;ï¼ˆè¶…çº§å–œæ¬¢è§†é¢‘çš„è¿™å¼ æˆªå›¾å˜»å˜»&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-09-06-p3.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-8&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;6å·-7å·å’Œå°ä¼™ä¼´ä»¬åèˆ¹å»èŠ¬å…°ï¼Œçœ‹æµ·çœ‹æ—¥è½æ—¥å‡ºã€‚åŸæ¥iPhoneè‡ªå¸¦çš„emojiçš„ğŸŒ„ğŸŒ…æ˜¯çœŸå®çš„åœºæ™¯å•Šï¼æ·±è“è‰²çš„å¤©ç©ºé€æ¸è¢«é‡‘è‰²æ·¹æ²¡ï¼Œæ³¢å…‰ç²¼ç²¼çš„æµ·é¢ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;center&gt;
&lt;video id=&quot;video&quot; controls=&quot;&quot; preload=&quot;none&quot;&gt;
&lt;source id=&quot;mp4&quot; src=&quot;/img/in-post/2019-09-08-video.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;2019-9-11&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;åˆå¼€å§‹é™·å…¥èµ·ä¸æ¥çš„å¾ªç¯äº†ï¼æ˜æ˜å¾ˆæ—©å°±ç¡ä½†æ˜¯è¿˜æ˜¯ä¼šç¡å¾ˆä¹…ï¼ğŸ˜¢ï¼ˆå•ªå•ªå•ªæ‰“è„¸&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ä»Šå¤©ä¸Šå•†ä¸šè¯¾å¬åˆ°ä¸€ä¸ªå¾ˆæœ‰è¶£çš„æ¡ˆä¾‹ï¼ŒæŸä¸ªç½‘ä¸Šçœ¼é•œåº—çš„glassesçš„é”€é‡è¿œå°äºlensesï¼Œä»–ä»¬èŠ±äº†å¾ˆä¹…æ‰¾äº†å¾ˆå¤šåŸå› ï¼ŒåŒ…æ‹¬çœ¼é•œæ ·å¼ã€ä¸èƒ½è¯•æˆ´ã€å•†å“æ’åˆ—ã€ä»·æ ¼ç­‰ç­‰ï¼Œå¹¶ä¸”ä¸€ä¸€è§£å†³ï¼Œä½†æ˜¯ç»“æœè¿˜æ˜¯ä¸å°½äººæ„ã€‚ç›´åˆ°æœ€åä»–ä»¬ä¸€ç¾¤äººåœ¨å®ä½“çœ¼é•œåº—è¹²å®ˆäº†ä¸‰å¤©ä¹‹åæ‰å‘ç°ä¸€ä¸ªå¾ˆå°çš„åŒºåˆ«ï¼š&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Buying glasses is a â€œgroup decisionâ€&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;äºæ˜¯ä»–ä»¬åœ¨ç½‘ç«™ä¸­è®¾ç½®äº†ä¸€ä¸ªå¾ˆå°çš„åŠŸèƒ½ï¼Œå°±æ˜¯åœ¨æ¨¡æ‹Ÿè¯•æˆ´ä¹‹åå¯ä»¥ä¸€é”®è½¬å‘åˆ°ç¤¾äº¤å¹³å°ï¼Œæœä¸å…¶ç„¶ï¼Œé•œæ¡†çš„é”€é‡å¾—åˆ°äº†å¢åŠ ã€‚&lt;/p&gt;

    &lt;p&gt;è¿™è®©æˆ‘æƒ³åˆ°è®¸å¤šéç¤¾äº¤å‹åŠŸèƒ½APPéƒ½ä¼šæ…¢æ…¢çš„å¢åŠ è®¾ç½®ä¸€ä¸ªç¤¾äº¤åœˆçš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬è´­ç‰©ç½‘ç«™ã€ç…§ç‰‡/è§†é¢‘ç¼–è¾‘å™¨ã€éŸ³ä¹æ’­æ”¾è½¯ä»¶ã€ç”šè‡³è¿ä¸€äº›æ”¯ä»˜(bao)è½¯ä»¶æ›¾ç»ä¹Ÿä¸€åº¦æƒ³å……åˆ†åˆ©ç”¨å¼€å‘å…¶ç¤¾äº¤åŠŸèƒ½ï¼Œå› ä¸ºäººå¤§å¤šæ•°æ—¶å€™åšçš„å†³ç­–å°±æ˜¯â€group decisionâ€ï¼Œç¤¾äº¤åœˆå°±æ˜¯äº’è”ç½‘å…¬å¸çš„ä¸€å¤§ç›ˆåˆ©æ¨¡å¼ã€‚&lt;/p&gt;

    &lt;p&gt;ç„¶åä¸ç»æ„Ÿæ…¨ä¸€ä¸‹å¾®ä¿¡çš„æˆåŠŸï¼Œä»ç®€å•çš„èŠå¤©è½¯ä»¶ï¼Œåˆ°æå‡ºè™šæ‹Ÿçº¢åŒ…æ…¢æ…¢æ¸—é€æ”¯ä»˜åŠŸèƒ½ï¼Œç”šè‡³èµ¶è¶…æ”¯ä»˜å®ï¼›ç„¶ååˆ°æç®€å°ç¨‹åºçš„æ¨å‡ºï¼Œä»¥ç¤¾äº¤åœˆä¸ºè½½ä½“èåˆæ‰€æœ‰åŠŸèƒ½å‹è½¯ä»¶ã€‚è¿™ç§è·³å‡ºä¹ æƒ¯æ€§æ€ç»´åœˆç›´å‡»è¦å®³çš„äº§å“å‘å±•è·¯çº¿ï¼Œè®©å¾®ä¿¡çœŸçš„åšåˆ°äº†ï¼š&lt;strong&gt;ä¸åªæ˜¯ä¸€ä¸ªèŠå¤©å·¥å…·ï¼Œæ›´æ˜¯ä¸€ç§ç”Ÿæ´»æ–¹å¼&lt;/strong&gt;ã€‚ä¸€å‘æ„Ÿè§‰æµæ—¥è®°å†™æ‰‹æˆ‘åªèƒ½æš—è‡ªè¯´ä¸€å¥â€œå¼ å°é¾™ç‰›é€¼â€ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-13 -&amp;gt; 2019-9-16&lt;/strong&gt; ğŸ‡®ğŸ‡¸&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ä¸¤å¤©å››å­£&lt;/p&gt;

    &lt;p&gt;æ¥å†°å²›å‰å‘ç°è¿ç»­ä¸¤å¤©éƒ½æ˜¯100%å¤§é›¨å¤©ï¼Œäºæ˜¯åšå¥½äº†å¿ƒç†å‡†å¤‡è¿æ¥ä¸€ä¸ªæš´é£é›¨çš„å†°å²›ï¼Œä½†æ˜¯å‘¨å…­å±…ç„¶ä¸€è·¯èµ°ä¸€è·¯æ”¾æ™´ï¼Œä¸€å¤©çœ‹åˆ°äº†ä¸‰å››ä¸ªå¤§å°ä¸ä¸€çš„å½©è™¹ğŸŒˆï¼ŒçœŸçš„å¤ªå¹¸è¿äº†ï¼Œè™½ç„¶å‘¨æ—¥å¦‚çº¦ä¸‹äº†é›¨ã€‚æˆ‘ä»¬æˆç§°çœŸçš„æ˜¯åœ¨ä¸¤å¤©ç»å†äº†å†°å²›çš„å››å­£ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;é»‘æ²™åœ°å’Œè‹”è—“&lt;/p&gt;

    &lt;p&gt;æ²¿ç€å—è¾¹æµ·å²¸çº¿çš„å…¬è·¯ä¸€è·¯å‘ä¸œæ˜¯å¤§ç‰‡çš„é»‘æ²™åœ°ï¼Œå¤šç«å±±çš„ç‰¹æ€§ä»¥åŠå†·å´çš„å²©æµ†çŸ³å’Œç«å±±ç°è®©è¿™è¾¹åœŸåœ°å¾ˆéš¾é•¿å‡ºç»¿è‰²çš„æ¤è¢«æ¥ã€‚æ¯å¹´ä¼¼ä¹åªèƒ½é•¿3cmçš„è‹”è—“æˆäº†è¿™ç‰‡åœŸåœ°å”¯ä¸€çš„æ‹“è’è€…ï¼Œåœ¨æ¯ä¸€æ¬¡ç«å±±çˆ†å‘ä¹‹ååˆé‡æ–°å¼€å§‹ç”Ÿé•¿ï¼Œå‘¨è€Œå¤å§‹ã€‚&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-09-17-blacksand.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-09-17-moss.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Human habitat&lt;/p&gt;

    &lt;p&gt;è½¦ä¸Šä¸­é€”é†’æ¥åï¼ŒLinè¯´åˆ°ï¼Œè¿™æ˜¯å¥¹ç¬¬ä¸€æ¬¡ä»å¸‚æ°‘å£ä¸­å¬åˆ°â€œHuman habitatâ€è¿™æ ·çš„å­—çœ¼ï¼Œå‘å¯¼æŠŠå¥¹ä»¬å±…ä½åœ¨å†°å²›è¿™ç‰‡åœŸåœ°ç§°ä¹‹ä¸ºâ€œHuman habitatâ€ã€‚ç”Ÿå­˜åœ¨è¿™ç‰‡æ‹¥æœ‰ç«å±±ã€ç€‘å¸ƒã€è¿œå¤å†°å·ç­‰è¯¸å¤šåŸå§‹åœ°è²Œçš„åœŸåœ°ä¸Šï¼Œå¥¹ä»¬ä¾æ—§å¯¹è‡ªç„¶å……æ»¡æ•¬ç•ï¼Œæ¯”èµ·dominateè¿™ç‰‡åœŸåœ°ï¼Œåªæ˜¯æˆ–è®¸ä¹Ÿåªèƒ½æ˜¯habitateã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æœ‰è¶£&lt;/p&gt;

    &lt;p&gt;ä¸çŸ¥é“è¿™æ˜¯ä¸æ˜¯å†°å²›äººç‹¬æœ‰çš„ç‰¹ç‚¹ï¼šæ¯ä¸ªåœ°æ–¹çš„æŒ‡ç¤ºæ ‡å¿—éƒ½å„ä¸ç›¸åŒï¼Œå¹¶ä¸”ç”»çš„ååˆ†æœ‰è¶£å“ˆå“ˆå“ˆã€‚å¥½åƒè®¾è®¡ä¸åŒçš„æ ‡å¿—ç¬¦å·å°±æ˜¯ä»–ä»¬åœ¨æ¼«é•¿å†¬æ—¥é‡Œçš„æ¶ˆé£ä¹‹ä¸€ã€‚è®©æˆ‘å¿½ç„¶è”æƒ³åˆ°é«˜æ™“æ¾åœ¨æ™“è¯´é‡Œä¼¼ä¹æåˆ°çš„ä¸€ä¸ªè§‚ç‚¹ï¼Œå¤§æ¦‚æ˜¯é‚£äº›ç»å†è¿‡æ¯”è¾ƒå¤šç£¨éš¾åœ°åŒºçš„äººæ°‘å¤šåŠæ›´æœ‰è¶£ä¸€äº›ï¼ˆå½“æ—¶ä¸¾çš„ä¾‹å­æ˜¯ä¸œåŒ—åœ°åŒºï¼‰ã€‚æ„Ÿè§‰æœ‰äº›åº”æ™¯å“ˆå“ˆ&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-09-17-signal.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æƒ³åˆ°å†ç»§ç»­å†™å§ï¼highly recommendï¼hahahahah&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-17&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;å› ä¸ºã€ŠGEBã€‹åˆ°äº†æˆ‘å®Œå…¨è¯»ä¸ä¸‹å»çš„ç¯èŠ‚ï¼ˆå¯¹ä½è—å¤´è¯—ä¸ä¸€è‡´æ€§ã€å®Œå…¨æ€§ä¸å‡ ä½•å­¦ï¼‰ï¼Œäºæ˜¯æ‰“å¼€äº†ä¸€æœ¬æ–°ä¹¦ã€Šä¹Œåˆä¹‹ä¼—ï¼šå¤§ä¼—å¿ƒç†ç ”ç©¶ã€‹ï¼Œåˆšçœ‹çš„æ—¶å€™è”æƒ³åˆ°è„±æ¬§å…¬æŠ•å’Œä¸€ç³»åˆ—ç¤¾äº¤åª’ä½“ä¸Šçš„åæ™ºè¡Œä¸ºåŸæœ¬ä»¥ä¸ºè¿™æ˜¯ä¸€æœ¬æ¯”è¾ƒæ–°çš„ä¹¦ï¼Œç»“æœå‘ç°ç«Ÿç„¶æ˜¯å‡ºç‰ˆäº1895å¹´ï¼ä¸å¾—ä¸æ„Ÿæ…¨äºä¸€ç™¾å¤šå¹´å‰å°±æœ‰äººèƒ½å¤Ÿå®Œç¾é¢„æµ‹æ¦‚æ‹¬ç¾¤ä½“å¿ƒç†çš„ç‰¹å¾ï¼Œè™½ç„¶åˆ°ç›®å‰ä¸ºæ­¢ä»–è¿˜ä¸å¤Ÿå®Œå…¨å……åˆ†çš„å‘æˆ‘justifyä¸€ä¸ªç¾¤ä½“çš„æƒ…ç»ªåŒ–ã€æ— å¼‚è®®ã€ä½æ™ºå•†ç­‰ç‰¹å¾çš„åˆç†æ€§ï¼Œä¸”æ„Ÿè§‰æ€ä¹ˆæ˜æ˜ç®€ç®€å•å•çš„è¯å´è¢«ç¿»æ¥è¦†å»çš„ç»•ç€è¯´ï¼Œé˜…è¯»ä½“éªŒä¸æ˜¯hinå¥½ï¼Œä½†æ˜¯ä¸”çœ‹å§å“ˆå“ˆ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;å¥½å¤šå¤©æ²¡æœ‰æ™¨è·‘é”»ç‚¼äº†ï¼ï¼ï¼åˆ«å¿˜äº†Flagï¼ï¼ï¼&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-18&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;æœ€è¿‘çœ‹the story of usæœ‰æ„Ÿï¼šæ¯”èµ·ä¸€ä¸ªå›½å®¶ï¼Œæ„Ÿè§‰ç¾å›½æ›´åƒæ˜¯ä¸€ä¸ªå…¬å¸ï¼Œä¸€ä¸ªå…¸å‹çš„entreprenureshipçš„æˆåŠŸæ¡ˆä¾‹ã€‚æœ€æ—©â€œå¤´è„‘é£æš´â€çš„â€œåˆ›å§‹äººâ€ä¹‹ä¸€John Rolfeå°±æ˜¯ä¸€ä¸ªä¸æŠ˜ä¸æ‰£çš„entrepreneurï¼Œå‘ç°çƒŸè‰ç”Ÿæ„åœ¨ç¾æ´²è¿™å—æ–°å¤§é™†ä¸Šçš„å¯å®æ–½æ€§ï¼Œéšåè¿›è¡Œæ‰©å¼ ã€é€æ¸å‘å±•ã€‚&lt;/li&gt;
  &lt;li&gt;ä½†æ˜¯ï¼Œä¸€ä¸ªæ‹¿â€œall men are created &lt;strong&gt;equal&lt;/strong&gt;, that they are endowed by their Creator with certain unalienable Rights, that among these are &lt;strong&gt;Life&lt;/strong&gt;, Liberty, and the pursuit of Happinessâ€ä½œä¸ºå®£è¨€çš„å›½å®¶å´èµ·å®¶äºå›é€ƒè‡ªå·±çš„å›½å®¶å é¢†æ å¤ºåˆ«äººçš„åœŸåœ°ç”šè‡³å± æ€é‚£å—åœŸåœ°ä¸Šçš„äººæ°‘ï¼Œè¿™ç§æè‡´çš„åŒæ ‡è¿˜èƒ½æŠŠäººæ´—è„‘è®¤ä¸ºè‡ªå·±æ˜¯fight for freedomçœŸçš„æ˜¯ç–‘æƒ‘æ“ä½œï¼ˆè§‚å¿µä¸æ–­æ›´æ–°ä¸­æˆ–è®¸æ˜å¤©å°±æ‰“è„¸ã€‚ã€‚ã€‚æ±‚ç”Ÿæ¬²å¾ˆå¼ºï¼‰&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-21&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;å—¯ï¼&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-24&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;æ„Ÿè§‰è‡ªå·±è¿è½å¤«è¿™ä¸ªç§°å‘¼éƒ½ä¸é…äº†&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-9-25&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;æ¥æ¬§æ´²ä¹‹åæˆ‘ä¸€ç›´å¾ˆç–‘æƒ‘çš„ç‚¹åœ¨äºæ˜¯ä»€ä¹ˆèƒ½å¤Ÿæ”¯æ’‘ä¸€ä¸ªå¦‚æ­¤é«˜ç¦åˆ©å´â€ä½â€æ•ˆç‡çš„å›½å®¶ä¸€ç›´ç¨³é€Ÿå‘å±•ï¼Œæˆ–è®¸å¾ˆå¤šäººä¼šæåˆ°è¯´å› ä¸ºæ¬§æ´²çš„å‘å±•æ¨¡å¼æ˜¯å¯æŒç»­çš„è¯¸å¦‚æ­¤ç±»çš„è¯ï¼Œé‚£ä¸­å›½å¦‚ä»Šè¿™ç§è¢«è®¸å¤šäººè¯Ÿç—…â€œä¸æŒç»­â€çš„å‘å±•æ¨¡å¼æ˜¯å¿…ç„¶çš„å—ï¼Œæ˜¯ç°æœ‰çº¦æŸæ¡ä»¶ä¸­çš„æœ€ä¼˜è§£å—ï¼Ÿ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;åˆä¸€æ¬¡è®©æˆ‘æƒ³åˆ°è¿™ä¸ªé—®é¢˜çš„èµ·æºæ˜¯ä»Šå¤©å•†ä¸šè¯¾ä¸Šå±•ç¤ºçš„workshopï¼Œä»»åŠ¡æ˜¯å»å‘¨å›´çš„å•†åœºé€›ä¸€åœˆç„¶åæå‡ºä¸€ä¸ªå•†ä¸šidea å•Šã€‚ã€‚å¥½ä¹…æ²¡å†™æˆ‘å¿˜æƒ¹ï¼ï¼&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;8hello-sweden&quot;&gt;ã€Œ8ã€Hello, Sweden&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;2019-8-22&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;æ˜¨å¤©å’Œå‰å¤©çœ‹äº†ä»Šæ•çš„ã€Šä¸œäº¬æ•™çˆ¶ã€‹å’Œã€Šçº¢è¾£æ¤’ã€‹ï¼Œæ„Ÿæ…¨äºå¤§å¸ˆå¯¹äºé•œå¤´çš„è¿ç”¨ï¼ˆæˆ–è®¸å¯¹äºæ¼«ç”»ä¸åº”è¯¥è¿™ä¹ˆè¯´ï¼Ÿå¤–è¡Œå¤–è¡Œï¼‰&lt;/p&gt;

    &lt;p&gt;æœ€å–œæ¬¢çš„é•œå¤´ä¹‹ä¸€æˆ–è®¸å°±æ˜¯ã€Šä¸œäº¬æ•™çˆ¶ã€‹é‡Œå“ˆå¨œå’Œå°é’å¦–è¿™ä¸ªèƒŒå½±çš„é‡å äº†å§ï¼&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-22-p1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;æœ€å–œæ¬¢çš„ç‰‡æ®µåº”è¯¥æ˜¯ã€Šçº¢è¾£æ¤’ã€‹é‡Œçš„æ¸¸è¡Œäº†å§ï¼Œå°¤å…¶æœ‰èƒŒæ™¯éŸ³ä¹çš„åŠ æŒæ¯ä¸€æ¬¡çœ‹éƒ½ä¼šèµ·é¸¡çš®ç–™ç˜©ï¼&lt;/p&gt;

    &lt;p&gt;å¯èƒ½è¦æ„Ÿè°¢ä»–ï¼Œè®©è¿‘ä¸€å¹´æŒç»­ç„¦è™‘åˆ°æœ€è¿‘ç”šè‡³è¿ä¸€é›†ç»¼è‰ºéƒ½éš¾ä»¥è®¤çœŸçœ‹ä¸‹å»çš„æˆ‘å®‰å®‰å¿ƒå¿ƒåœ¨å®¶çœ‹å®Œäº†ä¸¤éƒ¨ç”µå½±ï¼Œå¾ˆå¤šç»†å°çš„ç‰‡æ®µéƒ½èƒ½è®©æˆ‘è”æƒ³åˆ°å¾ˆå¤šåæ¥çš„ç”µå½±ï¼Œæ„Ÿè§‰æ¯ä¸€ä¸ªå°ç‚¹å°±èƒ½è¢«æ‹å‡ºæ¥æ‹æˆä¸€éƒ¨å¥½ç”µå½±ï¼Œè®©æˆ‘è”æƒ³åˆ°äº†é«˜æ™“æ¾è¯„è®ºåˆ˜æ…ˆæ¬£çš„ã€Šä¸‰ä½“ã€‹ï¼ŒæŠŠä¸€åˆ‡å¥½ä¸œè¥¿éƒ½ç»Ÿç»Ÿæ‘†å‡ºæ¥ç»™ä½ çœ‹ã€‚æˆ–è®¸å°±æ˜¯è¿™æ ·é«˜å¯†åº¦çš„è„‘æ´è¾“å‡ºæ‰ä¼šè®©äººçœ‹çš„ç•…å¿«æ·‹æ¼“å§ï¼ï¼ˆæˆ‘å¯çœŸæ˜¯ä¸ªæ„Ÿå—æ´¾æµæ°´è´¦è®°å½•è€…å“ˆå“ˆå“ˆå“ˆ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ä»¥åŠæœ€è¿‘åœ¨æ…¢æ…¢çœ‹ã€ŠGEBã€‹ï¼Œæˆ‘å¯å¤ªå–œæ¬¢ä¸­é—´çš„æ€ªåœˆç†è®ºäº†ï¼ä»¥åŠå…¶ä¸­æåˆ°çš„å¤§å¤šæ•°çŸ›ç›¾çš„æ¥æºæ˜¯â€œè‡ªæŒ‡æ€§â€&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;æ‰€è°“â€œæ€ªåœˆâ€ç°è±¡ï¼Œå°±æ˜¯å½“æˆ‘ä»¬å‘ä¸Šï¼ˆæˆ–å‘ä¸‹ï¼‰ç©¿è¿‡æŸç§å±‚æ¬¡ç³»ç»Ÿä¸­ä¸€äº›å±‚æ¬¡æ—¶ï¼Œä¼šæ„å¤–çš„å‘ç°æˆ‘ä»¬æ­£å¥½å›åˆ°äº†æˆ‘ä»¬å¼€å§‹çš„åœ°æ–¹ã€‚&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;

    &lt;blockquote&gt;
      &lt;ul&gt;
        &lt;li&gt;ä»»ä½•ä¸€ä¸ªå½¢å¼ç³»ç»Ÿï¼Œåªè¦åŒ…æ‹¬äº†ç®€å•çš„åˆç­‰æ•°è®ºæè¿°ï¼Œè€Œä¸”æ˜¯è‡ªæ´½çš„ï¼Œå®ƒå¿…å®šåŒ…å«æŸäº›ç³»ç»Ÿå†…æ‰€å…è®¸çš„æ–¹æ³•æ—¢ä¸èƒ½è¯æ˜çœŸä¹Ÿä¸èƒ½è¯ä¼ªçš„å‘½é¢˜ã€‚&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;æƒ³åˆ°é«˜ä¸­ç‰©ç†è€å¸ˆæ¯æ¯è®²åˆ°æ¿€æƒ…æ˜‚æ‰¬å¤„éƒ½ä¼šæåˆ°çˆ±å› æ–¯å¦æ™šå¹´æƒ³éªŒè¯å¹¿ä¹‰ä¸ç‹­ä¹‰ç›¸å¯¹è®ºæ˜¯èƒ½å¤Ÿèåˆçš„ï¼Œå½¢æˆä¸€å¥—é—­åˆå®Œå¤‡çš„ç†è®ºï¼Œæ‰€ä»¥è¯´Ph.D = Doctor of Philosophy ä¸æ˜¯æ²¡æœ‰åŸå› çš„å•Šï¼&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-24&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;â€œæ²¡èƒ½ç»§ç»­çš„è¯—ç¯‡ ï¼Œä¸æ¬¢è€Œæ•£çš„å‘Šåˆ«â€&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-25&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ä»Šå¤©ç»ˆäºæŠŠã€Šåƒå¹´å¥³ä¼˜ã€‹ä¹Ÿçœ‹å•¦ï¼
    &lt;ul&gt;
      &lt;li&gt;è®©æˆ‘æƒ³åˆ°äº†ã€Šæš—æ‹æ¡ƒèŠ±æºã€‹çš„å‰§æƒ…ï¼ˆè™½ç„¶æˆ‘è¿˜æ²¡æœ‰çœ‹è¿‡æš—æ‹åªçœ‹è¿‡å‘å¾€çš„ç”Ÿæ´»é‡Œé‚£ä¸€å°æ®µå˜»å˜»ï¼‰&lt;/li&gt;
      &lt;li&gt;å†ä¸€æ¬¡æ„Ÿæ…¨ä¸€ä¸‹ä»Šæ•çš„å‰ªè¾‘å’Œé•œå¤´ï¼Œåœ¨ç°å®å’Œè™šå¹»ä¸­éšæ„ç©¿æ¢­ï¼›ä»¥åŠèƒ½æŠŠå¾®å°çš„æƒ…æ„Ÿæ‹å‡ºå®å¤§çš„å®‡å®™æ„Ÿ&lt;/li&gt;
      &lt;li&gt;æœ€å–œæ¬¢çš„ã€Šè€Œæˆ‘çŸ¥é“ã€‹çš„MVä¸­çš„ç»“å°¾â€œç°åœ¨æˆ‘çŸ¥é“ï¼Œæˆ‘çˆ±ä¸Šçš„æ˜¯17å²çš„ï¼Œæˆ‘è‡ªå·±â€&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æ™šä¸Šå»Stockholm City Hallå‚åŠ å¼€å­¦å…¸ç¤¼ï¼Œå°±æŠ½ä¸‹åˆå‡ºé—¨å»äº†modern museumï¼Œæƒ³å»çœ‹é©¬è’‚æ–¯ï¼æ¯•ç«Ÿä»–çš„ã€ŠThe Joy of Lifeã€‹å æ®äº†æˆ‘å¤šå¹´æœ‹å‹åœˆçš„èƒŒæ™¯ï¼Œè¿˜æƒ³è¿‡è¦çº¹èˆè¹ˆï¼å¯æƒœä¸å‡ºæ„å¤–çš„æ²¡æœ‰çœ‹åˆ°&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-25-p1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;ä¸è¿‡æ„å¤–æ‹åˆ°äº†å¾ˆçˆ±çš„è™šå®ç›¸ç”Ÿçš„é£æ™¯ï¼ğŸ‘‡ é‚£æˆ‘å°±å–åã€Šå¼€ç€çš„çª—æˆ·ã€‹å§ credit to Matisse&lt;/p&gt;

    &lt;p&gt;æ¯•ç«Ÿå½“ä»£è‰ºæœ¯ä¸å°±æ˜¯è¿½æ±‚â€œäººäººéƒ½æ˜¯è‰ºæœ¯å®¶â€å—ï¼å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆ&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-25-p2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;ä»å¼€å­¦å…¸ç¤¼å›æ¥çš„è·¯ä¸Šæ•´ä¸ªåŸå¸‚éƒ½è¢«ç²‰ç´«è‰²çš„äº‘æœµç¬¼ç½©ï¼Œå¤ªç¾äº†ï¼ˆæ‰‹æœºåªèƒ½è¡¨è¾¾å‡ºæ¥ååˆ†ä¹‹ä¸€çš„ç¾ï¼åä¸ºå‹‰å¼ºååˆ†ä¹‹äºŒï¼ï¼&lt;/p&gt;

    &lt;p&gt;æˆ‘å’ŒLinä¸€è·¯å¥”è·‘ï¼Œå»è¿½èµ¶lidlå…³é—¨å‰çš„ï¼Œå’Œå¥¹çš„â€familyâ€æ¥è§å¥¹è·¯ä¸Šçš„20åˆ†é’Ÿï¼Œæˆ‘è°ƒä¾ƒï¼Œâ€œå’±ä»¬è¿™æ˜¯æ–¯å¾·å“¥å°”æ‘©è·‘å•Šâ€ï¼Œäºæ˜¯è¶Šè·‘è¶Šèµ·åŠ²ï¼Œä¸¤ä¸ªäººå‚»ä¹ä¹çš„åœ¨è¡—ä¸Šå¤§ç¬‘&lt;/p&gt;

    &lt;p&gt;ä¸€è·¯è·‘è¿‡ï¼Œå´åˆä¸€è·¯å¿µå¿µä¸èˆçš„æ‹ç…§ï¼ŒLinè¯´ï¼Œä¸€å®šè¦æŠŠå®ƒç•™ä¸‹å‘€&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-25-p3.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;æŠµæŠ—ç„¦è™‘çš„è‰¯æ–¹ï¼Œæˆ–è®¸å°±æ˜¯å……å®å®‰å¿ƒçš„è¿‡å¥½æ¯ä¸€å¤©ã€Œç”Ÿæ´»ã€å§ï¼Get a Life! ï¼ˆæ‰£é¢˜æ»¡åˆ†&lt;/p&gt;

    &lt;p&gt;è…¿çœŸé…¸å‘€ï¼æ™šå®‰ï¼&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-26&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ç¬¬ä¸€å¤©ä¸Šè¯¾å•¦ï¼Œå•†ç§‘è¯¾ç¨‹çš„è€å¸ˆæ˜¯ä¸ªæœ‰è¶£çš„å…ˆç”Ÿï¼Œè¿è¿èµå¹ä»–çš„pitchæŠ€å·§&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-27&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;èµ–äº†åŠå°æ—¶åºŠä¹‹åèµ·åºŠæ™¨è·‘(èµ°)&lt;/p&gt;

    &lt;p&gt;å¬è¯´æ²¿ç€å­¦æ ¡èƒŒåçš„å°æ£®æ—ä¸€è·¯ç©¿è¿‡å»ä¼šé‡åˆ°ä¸€ç‰‡æ¹–ï¼Œå¾ˆå¥½å¥‡ï¼Œäºæ˜¯æ…¢æ…¢æ‚ æ‚ è¿è·‘å¸¦èµ°ï¼Œç©¿è¿‡äº†æ–¯å¤§çš„æ ¡å›­ï¼Œç»“æœèµ°åˆ°äº†lappisã€‚&lt;/p&gt;

    &lt;p&gt;é€”ä¸­å¶é‡äº†ç§‹åƒï¼Œæƒ³åˆ°ä¸Šæ¬¡11ç‚¹å¤šåœ¨æ»¨æ±Ÿå…¬å›­è¿˜ç©ä¸åˆ°çš„ç§‹åƒï¼ŒäºŒè¯ä¸è¯´å°±å†²äº†ä¸Šå»ã€‚æˆ‘çœŸçš„æœ‰çœ‹åˆ°é‡‘è‰²çš„é˜³å…‰å“¦&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-27-p2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;ç»ˆäºåˆ°è¾¾äº†æ‰€è°“çš„æŸä¸ªæ¹–è¾¹ï¼Œéšæ‰‹æ‹çš„ä¸¤å¼ ç…§ç‰‡åœ¨ç›¸å†Œé‡Œä¹ä¸€çœ‹åƒæ˜¯ä¸€å¼ é•¿å›¾ï¼Œwhat a coincidence!&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-27-p1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ç”©é”…è¿™ä»¶äº‹ï¼Œåœ¨ä¸çŸ¥ä¸è§‰ä¸­æˆ‘ä¹Ÿèƒ½å®Œæˆâœ…çš„éå¸¸ä¹‹å¥½ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;â€œå¹¸ç¦å½“ç„¶æ˜¯å¾ˆç¾å¥½çš„ï¼Œä½†æ˜¯ç¬é—´æ€ä¹ˆè¢«æ”¾å¤§å’Œå»¶ä¼¸ï¼Œæˆ‘æ²¡æœ‰å­¦è¿‡ï¼Œæˆ‘æ€•æˆ‘åšä¸åˆ°ã€‚â€&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-28&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ä¸Šè¯¾æ—¶å¿½ç„¶æƒ³åˆ°çš„ä»Šæ—¥ä¸»é¢˜è¯ï¼š&lt;strong&gt;åº”è¿è€Œç”Ÿ&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;ä¸€ä¸ªäººçš„å‘½è¿å•Šï¼Œå½“ç„¶è¦é è‡ªæˆ‘å¥‹æ–—ã€‚ä½†ä¹Ÿè¦è€ƒè™‘å†å²çš„è¿›ç¨‹&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;æœ€è¿‘çœ‹çš„ä¹¦é‡Œæåˆ°ï¼Œ&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;Lean Startup is a new way of looking at the development of innovative new products that emphasizes &lt;strong&gt;fast iteration&lt;/strong&gt; and &lt;strong&gt;customer insight&lt;/strong&gt;, a huge vision, and great ambition, all at the same time.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ä¸Šåˆçœ‹çš„è®ºæ–‡æ˜¯æœ‰å…³user-generated contentå¯¹äºProduct Designçš„å½±å“ï¼Œåœ¨è®ºæ–‡é‡Œä¹Ÿä¸è°‹è€Œåˆçš„æåˆ°äº†ä¸€ç§two-stagesæ‰‹æ®µï¼šdesign-then-priceã€‚å³å…ˆdesign productï¼Œç„¶åæ ¹æ®user-generated contentï¼ˆä¾‹å¦‚è¯„è®ºåé¦ˆç­‰ï¼‰å®šä»·&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ä¸‹åˆçš„è¯¾ä¸Šï¼ŒHenrikåå¤æåˆ°ï¼Œä¸€æ˜¯Historyçš„é‡è¦æ€§ï¼ŒHistory is the best training set in order to predict futureï¼›äºŒæ˜¯logicï¼Œéœ€è¦å¯¹äº‹ç‰©è¿½æ ¹æº¯æºå¯»æ‰¾å…¶å†…åœ¨é€»è¾‘å’Œå‘å±•è§„å¾‹ï¼ˆè«åæ­£ç»ï¼‰ã€‚ä»–ä¸¾åˆ°äº†marketingå’Œadvertisingäº§ç”Ÿçš„åŸå› ï¼Œä¾›éœ€è§’è‰²çš„å€¾æ–œå¯¼è‡´ç”±calling to buyåˆ°marketingçš„å˜åŒ–ï¼Œä¹Ÿæ˜¯push -pullçš„è½¬æ¢ï¼ˆå¤§ä¸€å­¦çš„MISå¿½ç„¶è¹¦å‡ºè„‘æµ·ï¼‰ã€‚ç§‘æŠ€çš„è¿…é€Ÿå‘å±•ï¼Œäº’è”ç½‘ä¿¡æ¯çˆ†ç‚¸ï¼Œäº§å“æ›´æ–°æ¢ä»£é€Ÿåº¦çš„æŒ‡æ•°æ¿€å¢ï¼Œå¯¼è‡´ä¼ ç»Ÿçš„éƒ¨åˆ†product designæ¨¡å¼æ— æ³•é€‚ç”¨ã€‚Fast iteration is required.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;æƒ³åˆ°äº†æ—©ä¸Šçœ‹çš„Bayesian learning
    &lt;blockquote&gt;
      &lt;p&gt;In other words, if we have enough data, we see that the data overwhelms the prior.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;æ¯”èµ·å®Œç¾çš„åˆ»ç”»å‡ºå…ˆéªŒçŸ¥è¯†ï¼Œåœ¨dataè¶³å¤Ÿå¤šçš„æƒ…å†µä¸‹ï¼ˆå½“ä¸‹ï¼‰ï¼Œä¸æ–­çš„æ”¶é›†ç”¨æˆ·ä¿¡æ¯ç­‰è¿›è¡Œè¿­ä»£æ›´åŠ é‡è¦&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æ¯•åŠ ç´¢çš„ç«‹ä½“ä¸»ä¹‰ä¹ƒè‡³å½“ä»£è‰ºæœ¯è¯ç”Ÿäºå½±åƒæŠ€æœ¯é€æ¸æˆç†Ÿçš„æŠ€æœ¯å¤§è¿›æ­¥æ—¶æœŸï¼Œæœºæ¢°å¤åˆ¶æ—¶ä»£çš„åˆ°æ¥å¼±åŒ–äº†è‰ºæœ¯å¯¹äºå†™å®çš„éœ€æ±‚ï¼Œè€Œè¡¨è¾¾ä¸æ€è€ƒæˆäº†ä¸»è¦ç›®çš„ã€‚&lt;/p&gt;

    &lt;p&gt;åœ¨æŸæ‹‰å›¾çš„ã€Šç†æƒ³å›½ã€‹ä¸€ä¹¦ä¸­æœ‰ä¸€ä¸ªéå¸¸ç»å…¸çš„æ¯”å–»â€”â€”â€œåºŠå–»â€ï¼Œå³å…³äºä¸‰å¼ åºŠçš„æ¯”å–»ã€‚ä»–è®¤ä¸ºç¬¬ä¸€å¼ åºŠæ˜¯ä¸€ä¸ªç”±ä¸Šå¸åˆ›é€ çš„ç†å¿µ(idea/concept)ï¼Œç¬¬äºŒå¼ åºŠæ˜¯å·¥åŒ ä¸ºäº†ä»¿ç…§ä¸Šå¸çš„æ¦‚å¿µè€Œåˆ¶é€ å‡ºçš„å®ç‰©ï¼Œç¬¬ä¸‰å¼ åºŠæ˜¯åˆ™è‰ºæœ¯å®¶ä¸ºäº†æ‘¹ä»¿å·¥åŒ æ‰€åˆ¶é€ çš„è€Œåˆ›ä½œçš„ä½œå“ã€‚
æŸæ°çš„åºŠå–»è¡¨æ˜ï¼Œè‡ªç„¶ä¹‹åºŠå³ç¥é€ ä¹‹åºŠï¼Œæ˜¯åŸå‹è§‚å¿µï¼Œä»£è¡¨çœ‹ä¸è§æ‘¸ä¸ç€çš„ç†å¿µæœ¬ä½“ï¼Œå±äºç†æ™ºä¸–ç•Œçš„å¯çŸ¥å¯¹è±¡ï¼Œæ˜¯ä¸–ç•Œæ‰€æœ‰åºŠçš„æœ¬æºï¼›æœ¨åŒ æ‰€é€ ä¹‹åºŠï¼Œæ˜¯ç”Ÿæ´»å™¨å…·ï¼Œå±äºç°è±¡ä¸–ç•Œçš„å¯è§†å¯¹è±¡ ï¼Œæ˜¯æ‘¹ä»¿åŸå‹ç†å¿µçš„äº§ç‰©ï¼Œä»£è¡¨å®ç”¨æŠ€è‰ºï¼›ç”»å®¶æ‰€ç”»ä¹‹åºŠæ˜¯æœ¨åŒ æ‰€é€ ä¹‹åºŠçš„æ‘¹æœ¬ï¼Œç›¸å¯¹äºç¥æ‰€é€ ä¹‹åºŠè€Œè¨€ï¼Œå°±æˆäº†æ‘¹æœ¬ä¹‹æ‘¹æœ¬ï¼Œå±äºè™šåŒ–å½±åƒï¼Œæ˜¯å½±å­ä¹‹å½±å­ï¼Œä¸åºŠçš„ç‰©ç†å®ä½“ç›¸éš”ä¸€å±‚ï¼Œæ²¡æœ‰å®ç”¨ä»·å€¼ï¼›ä¸åºŠçš„æœ¬ä½“ç›¸éš”ä¸¤å±‚ï¼Œæ²¡æœ‰è®¤è¯†ä»·å€¼ï¼Œåªèƒ½ä»£è¡¨æ‘¹ä»¿è‰ºæœ¯ã€‚&lt;/p&gt;

    &lt;p&gt;æœ‰è¶£çš„æ˜¯ï¼Œ1965å¹´çº¦ç‘Ÿå¤«Â·ç§‘è‹æ–¯çš„ä½œå“ã€Šone and three chairsã€‹ä¹Ÿå¾ˆå¥½çš„å†ç°äº†æŸæ‹‰å›¾çš„è¿™ä¸ªç†å¿µã€‚ç…§ç‰‡é‡Œçš„æ¤…å­ï¼Œæ˜¯å®ç‰©çš„å½±åƒï¼ŒçœŸå®çš„è¿™æŠŠæ¤…å­å‡ºè‡ªå·¥åŒ ä¹‹æ‰‹ï¼Œæ˜¯æ¨¡ä»¿ç†å¿µæœ¬ä½“çš„äº§ç‰©ï¼Œè€Œé‚£äº›å¯¹æ¤…å­çš„è§£é‡Šä¸æ­£å¥½å°±æ˜¯æ¤…å­çš„åŸå‹è§‚å¿µå—&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-28-p1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;å½“ä»£è‰ºæœ¯çš„å‘å±•å¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¯ç”±äºç§‘å­¦æŠ€æœ¯å‘å±•çš„æ¨åŠ¨ï¼Œæœºæ¢°å¤åˆ¶æ—¶ä»£è®©è‰ºæœ¯çš„å¤§é‡å¤åˆ¶æˆä¸ºå¯èƒ½ï¼Œè‰ºæœ¯ç›¸å¯¹äºäººä»¬æ¥è¯´ä¸å†æ˜¯ç¥ç§˜çš„å´‡æ‹œå¯¹è±¡ï¼Œè€Œå˜ä¸ºè§¦æ‰‹å¯åŠçš„å®¡ç¾å¯¹è±¡ï¼ŒæŠ€å·§æ€§çš„æç»˜ç¾çš„äº‹ç‰©ä»…ä»…åªèƒ½å¸¦ç»™è§‚ä¼—ç¾çš„è§‚æ„Ÿã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Coco Chanelï¼ˆæœ€å–œæ¬¢çš„å“ç‰Œä¹‹ä¸€å•Šå•Šå•Šï¼‰æ‰“ç ´äº†â€œå¸½å­ä¸Šæœ‰ä¸€ä¸ªèŠ±å›­â€çš„ä¼ ç»Ÿï¼Œä»¥ç®€æ´çš„æ— è£…é¥°è‰å¸½å‡ºåï¼›æ‘’å¼ƒäº†æŸè…°æ‹–åœ°é•¿è£™ï¼Œä»¥ç”·è£…ä¸ºçµæ„Ÿé‡‡ç”¨ç›´çº¿å‰ªè£ï¼ŒåŠè†ç›´ç­’è£™ç”šè‡³é•¿è£¤ï¼›è®¨åŒå¥³äººä»¬ç©¿çš„â€œäº”å½©ç¼¤çº·â€ï¼Œäºæ˜¯è®¾è®¡äº†å°é»‘è£™ï¼ˆè¶…è‡ªæˆ‘å‘€ï¼ï¼ï¼‰ï¼Œä½†è¿™ä¸€åˆ‡ä¼¼ä¹ä¹Ÿæ˜¯å› ä¸ºå¥¹æ•é”çš„æŠ“ä½äº†ï¼ˆæˆ‘ä»¬æ— ä»å¾—çŸ¥æ˜¯å¥¹é€‰æ‹©äº†è¿˜æ˜¯è¢«é€‰æ‹©äº†ï¼‰å¥³æ€§æ„è¯†çš„è§‰é†’è¿™ä¸€æ—¶ä»£ç‰¹å¾ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;ç»¼ä¸Šæ‰€è¿°ï¼Œè¦å¤šçœ‹ä¹¦å¤šçœ‹æŠ¥å‹¤å­¦ä¹ å‹¤æ€è€ƒå°‘åƒé›¶é£Ÿå¤šç¡è§‰ :)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-29&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ã€Šéæš´åŠ›æ²Ÿé€šã€‹ï¼š&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;ä½¿ç”¨æš´åŠ›çš„äººçš„å…¶å®æ˜¯å› ä¸ºä»–ä»¬å†…å¿ƒçš„å®é™é­åˆ°äº†ç ´åï¼Œæ‰€ä»¥ä»–ä»¬æ‰ä¼šæœ‰æš´åŠ›çš„æ–¹å¼ç»´æŠ¤æˆ–å¯»æ±‚å¿ƒçµçš„å’Œå¹³ã€‚è¿™æˆ–è®¸æ˜¯æš´åŠ›çš„è´è¶æ•ˆåº”å§ã€‚&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;è§‚å¯Ÿï¼ˆå®¢è§‚ï¼Œéè¯„ä»·ï¼‰ï¼Œæ„Ÿå—ï¼ˆéçœ‹æ³•ï¼‰ï¼Œéœ€æ±‚ï¼ˆæ˜ç¡®ç»†èŠ‚ï¼Œæ­£é¢ï¼‰ï¼Œè¯·æ±‚ï¼ˆå…·ä½“ï¼‰&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-30&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;é¢„æµ‹ vs æ¨æ–­&lt;/p&gt;

    &lt;p&gt;â€œåŒæ ·æ˜¯å¯»æ‰¾y=f(X)ä¸­çš„f(Â·)ï¼Œå¦‚æœä»…ä»…ä¸ºäº†&lt;strong&gt;é¢„æµ‹&lt;/strong&gt;Yï¼Œf(Â·)å¯ä»¥æ˜¯ä¸€ä¸ªé»‘ç®±å­ï¼›ä½†å¦‚æœæ˜¯ä¸ºäº†&lt;strong&gt;æ¨æ–­&lt;/strong&gt;X, yä¹‹é—´çš„å…³ç³»ï¼Œf(Â·)å°±éœ€è¦çŸ¥é“å…·ä½“å½¢å¼ã€‚æ ¹æ®åˆ†æçš„ç›®æ ‡æ˜¯é¢„æµ‹ã€æ¨æ–­ï¼Œè¿˜æ˜¯ä¸¤è€…å…¼å…·ï¼Œä¼°è®¡f(Â·)æ‰€é‡‡ç”¨çš„æ–¹æ³•å¯èƒ½æ˜¯ä¸åŒçš„ã€‚â€&lt;/p&gt;

    &lt;p&gt;æ‰€ä»¥ä¼ ç»Ÿçš„ç ”ç©¶æ–¹æ³•å¾—ä»¥ç»§ç»­ï¼Œä¸”æ·±åº¦å­¦ä¹ çš„å¯è§£é‡Šæ€§ä¸€ç›´è¢«ç ”ç©¶çš„åŸå› å°±æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä»…ä»…åšåˆ°100%å‡†ç¡®çš„é¢„æµ‹æ˜¯æ— æ³•æ”¹å˜å½“ä¸‹çš„ï¼Œåªæœ‰ç ”ç©¶å…¶ä¸­çš„logicæ‰èƒ½å¤Ÿä¿®æ­£å®ƒã€æŠŠæ§å®ƒã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æˆ–è®¸æ¯”èµ· &lt;strong&gt;__&lt;/strong&gt;_ï¼Œæˆ‘æ›´å¸Œæœ› &lt;strong&gt;__&lt;/strong&gt;__&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2019-8-31&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;å¤ªå–œæ¬¢è¿™éƒ¨ç”µå½±çš„è¡¨ç°æ‰‹æ³•äº†ï¼ï¼ï¼ï¼ˆçœ‹æ¥æˆ‘ç›®å‰è¿˜åœ¨é‚£ç§è§‰å¾—ä¸–ç•Œå“ªå„¿å“ªå„¿éƒ½å¥½çš„é˜¶æ®µå“ˆå“ˆå“ˆï¼‰ï¼Œè¶…ç°å®ä¸»ä¹‰çš„è¡¨ç°æ‰‹æ³•å¾ˆå®Œç¾çš„é…åˆäº†fridaçš„ç”»ï¼ï¼ˆä½†æ˜¯å¯¹äºfridaæ¥è¯´å¥¹å¹¶ä¸æ„¿æ„è®¤ä¸ºè‡ªå·±çš„ç”»æ˜¯è¶…ç°å®ä¸»ä¹‰çš„ï¼Œå¥¹æ˜¯åœ¨çœŸå®çš„æè¿°è‡ªå·±ï¼‰å¼‚åŸŸé£æƒ…çš„BGMä¹Ÿéå¸¸æŠ“æˆ‘ï¼Œç‹‚æ”¾ä¸”çƒ­çƒˆï¼Œå°¤å…¶æ˜¯æ¯æ¬¡fridaé†‰é…’ä¹‹åçš„ç‹‚æ¬¢ã€‚&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-31-p1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;ä»¥åŠè§‰å¾—ã€Šç‡•å°¾è¶ã€‹è¿™é¦–æ­Œå¾ˆé…å¥¹ï¼&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;æ”¯ç‚¹&lt;/strong&gt;ï¼šå› ä¸ºå¿½ç„¶æƒ³åˆ°ã€Šç‡•å°¾è¶ã€‹è¿™é¦–æ­Œæ˜¯é˜¿ä¿¡å› ä¸ºçœ‹äº†æŸä¸€éƒ¨å«ã€Šç‡•å°¾è¶ã€‹çš„ç”µå½±ä¹‹åå°±å¾ˆæƒ³çœ‹è¿™éƒ¨ç”µå½±ï¼Œç„¶åæƒ³åˆ°ä»¥å‰ç–¯ç‹‚å–œæ¬¢äº”æœˆå¤©çš„æ—¶å€™ï¼Œä¼šä»¥ä»–ä»¬ä¸ºæ”¯ç‚¹ï¼Œåˆ©ç”¨ä»…æœ‰çš„ä¸€éƒ¨æ™ºèƒ½æ‰‹æœºå´èƒ½å¤Ÿå‘å¤–ä¸æ–­å»¶ä¼¸ï¼Œå»çœ‹é‚£äº›å½±å“äº†é˜¿ä¿¡å†™è¯çš„å°è¯´ã€ç”µå½±ï¼Œå»å¬ä»–ä»¬å–œæ¬¢çš„æ‘‡æ»šä¹å›¢ï¼Œå»æ•å¼€è‡ªå·±å¿ƒæ‰‰åœ¨ç½‘è·¯ä¸Šä¸»åŠ¨è®¤è¯†æ–°æœ‹å‹ç”šè‡³èŠäººç”ŸèŠæœªæ¥ï¼ˆè™½ç„¶é‚£ä¸ªæ—¶å€™çš„äººç”Ÿç»éªŒæ¯”è¾ƒæµ…è–„å“ˆå“ˆå“ˆå“ˆï¼‰ï¼Œä¸ºäº†è§ä»–ä»¬å»è¿½å¯»ä¸€äº›ä¸æ•¢æƒ³çš„æ¢¦æƒ³è™½ç„¶æœ€åæ²¡èƒ½å®ç°ä½†ä¹Ÿpay offï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä¹ˆå……æ»¡æ­£èƒ½é‡çš„åé¦ˆå•Šï¼åè€Œç½‘é€Ÿè¶Šå¿«æ¸ é“èµ„æºè¶Šä¸°å¯Œæˆ‘å´æˆé•¿çš„è¶Šæ…¢äº†å•ŠçœŸæ˜¯ç¾æ„§ã€‚å†…å¿ƒä¸–ç•Œä¸€ç‰‡è´«ç˜ çš„äººæœç„¶æ´»çš„åƒæ˜¯è¡Œå°¸èµ°è‚‰æ¯«æ— å¯¹ç”Ÿæ´»çš„respectå’Œçƒ­æƒ…å•Šï¼Œæ‰€ä»¥è¯´äº”æœˆå¤©å½“å¹´æ˜¯ä¿¡ä»°çœŸçš„ä¸å‡å‘€ï¼ï¼ˆè¯´æ¥æœ‰ç‚¹çŸ«æƒ…ä¸”å®³æ€•ä¼¼ä¹ä¼šè¢«å¤§å®¶è®¤ä¸ºæ˜¯â€œä½çº§â€çš„ä¿¡ä»°ï¼‰è¿˜æƒ³åˆ°äº†è€å¸ˆæåˆ°è¿‡é‚£ä¸ªä¸ºäº†çƒ­çˆ±çš„ä¸“ä¸šå’¬ç‰™é€€å­¦äº†åˆç”³è¯·å‡ºå›½è¯»äº†è‡ªå·±å–œæ¬¢ä¸“ä¸šçš„å¥³å­©å­ï¼Œ&lt;strong&gt;â€œäººç”Ÿç»ˆå½’è¿˜æ˜¯è¦ one must be motivated by what she doesâ€&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 22 Aug 2019 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2019/08/22/Moments/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/22/Moments/</guid>
        
        <category>Get A Life</category>
        
        
      </item>
    
      <item>
        <title>[MLAPP] Chapter 2: Probability</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [ ['$$', '$$']],
            inlineMath: [['$','$']],
            processEscapes: true
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;h2 id=&quot;21-introduction&quot;&gt;2.1 Introduction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;What is probability?&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first interpretation is called the &lt;strong&gt;frequentist interpretation&lt;/strong&gt;. In this view, probabilities represent long run frequencies of events.&lt;/li&gt;
  &lt;li&gt;The other interpretation is called the &lt;strong&gt;Bayesian interpretation&lt;/strong&gt; of probability. In this view, probability is used to quantify our &lt;strong&gt;uncertainty&lt;/strong&gt; about something; hence it is fundamentally related to &lt;strong&gt;information&lt;/strong&gt; rather than repeated trials (Jaynes 2003).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;22-a-brief-review-of-probability-theory&quot;&gt;2.2 A brief review of probability theory&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Discrete random variables (probability mass function, abbreviated to pmf)&lt;/li&gt;
  &lt;li&gt;Fundamental rules, including Probability of a union of two events, Joint probabilities, Conditional probability&lt;/li&gt;
  &lt;li&gt;Bayes rule: Combining the definition of conditional probability with the product and sum rules yields Bayes rule, also called &lt;strong&gt;Bayes Theorem&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(X=x|Y=y) = \frac{p(X=x, Y=y)}{p(Y=y)} = \frac{p(X=x)p(Y=y|X=x)}{\sum_x' p(X=x')p(Y=y|X=x')}&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;Independence and conditional independence
    &lt;ul&gt;
      &lt;li&gt;X and Y are unconditionally independent or marginally independent:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;X \perp Y \Leftrightarrow p(X,Y) = p(X)p(Y)&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;X and Y are conditionally independent (CI) given Z iff the conditional joint can be written as a product of conditional marginals:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;X \perp Y|Z \Leftrightarrow p(X,Y|Z) = p(X|Z)p(Y|Z)&lt;/script&gt;
 &lt;strong&gt;Theorem 2.2.1.:&lt;/strong&gt; $X\bot Y |Z$ iff there exist function $g$ and $h$ such that $p(x, y|z) = g(x, z)h(y, z) $ for all $x,y,z$ such that $p(z) &amp;gt; 0$.&lt;/p&gt;

    &lt;p&gt;(! Need to be proved)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Continuous random variables(probability density function, abbreviated to pdf; cumulative distribution function, abbreviated to cdf)&lt;/li&gt;
  &lt;li&gt;Quantiles: denoted by $F^âˆ’1(\alpha)$&lt;/li&gt;
  &lt;li&gt;Mean(expected value), standard deviation and variance(a measure of the â€œspreadâ€ of a distribution)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;23-some-common-discrete-distributions&quot;&gt;2.3 Some common discrete distributions&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The binomial and Bernoulli distributions&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;binomial distribution $X \sim Bin(n,\theta)$:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;Bin(k|n,\theta) \triangleq  \binom{n}{k}\theta^k(1-\theta)^{n-k}&lt;/script&gt;

        &lt;p&gt;where $\binom{n}{k} \triangleq \frac{n!}{k!(n-k)!}$ is the number of ways to choose k items from n (this is known as the &lt;strong&gt;binomial coefficient&lt;/strong&gt;, and is pronounced â€œn choose kâ€).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Bernoulli distribution $X \sim Ber(\theta)$:&lt;/p&gt;

        &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
Ber(x|\theta) \triangleq  \theta^{\mathbb{I}(x=1)}(1-\theta)^{\mathbb{I}(x=0)} = \begin{cases}
 \theta &amp; \text{if x=1}\\
 1-\theta &amp; \text{if x=0}
 \end{cases} %]]&gt;&lt;/script&gt;
  Bernoulli distribution is the special case of a binomial distribution with $n=1$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;binomial distribution: How many times(k) do the coin face up when it is thrown for n times?&lt;/p&gt;

    &lt;p&gt;Bernoulli distribution: Do the coin face up when it is thrown for once?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The multinomial and multinoulli distributions&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;multinomial distributions:
  &lt;script type=&quot;math/tex&quot;&gt;Mu(\pmb{x}|n, \pmb{\theta} ) \triangleq  \binom{n}{x_1\cdots x_K}\prod_{j=1}^{K}\theta_j^{x_j}&lt;/script&gt;&lt;/p&gt;

        &lt;p&gt;where $\binom{n}{x_1\cdots x_K} \triangleq \frac{n!}{x_1!x_2!\cdots x_K!}$ is the &lt;strong&gt;multinomial coefficient&lt;/strong&gt; and $\sum x_i = n$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;multinoulli distributions(x is  its dummy encoding or one-hot encoding):
  &lt;script type=&quot;math/tex&quot;&gt;Cat(x|\pmb{\theta}) \triangleq Mu(\pmb{x}|1, \pmb{\theta} ) \triangleq \prod_{j=1}^{K}\theta_j^{\mathbb{I}(x_j=1)}&lt;/script&gt;
  This very common special case is known as a &lt;strong&gt;categorical or discrete distribution&lt;/strong&gt;. And $p(x = j|\pmb{\theta}) = \theta_j$&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;multinomial distribution: How many times for each surface ( $[x_1\cdots x_6]$ ) do a dice show when it is thrown for n times?&lt;/p&gt;

    &lt;p&gt;multinoulli distribution: What surface do a dice show when it is thrown for once?&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;Name&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;n&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;K&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;x&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;mean&lt;/th&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;variance&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Multinomial&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$x\in {0,1,\cdots, n}^K, \sum_{k=1}^{K}x_k=n$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$E(X_j) = n\theta_j$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$Var(X_j) = n\theta_j(1-\theta_j)$, $Cov(X_i, X_j) = -n\theta_i\theta_j$&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Multinoulli&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$x\in {0,1}^K, \sum_{k=1}^{K}x_k=1$(1-of-K encoding)&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$E(X_j) = n\theta_j$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$Var(X_j) = \theta_j(1-\theta_j)$, $Cov(X_i, X_j) = -\theta_i\theta_j$&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Binomial&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$x\in {0,1,\cdots, n}$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\theta$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$n\theta(1-\theta)$&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;Bernoulli&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$x\in {0,1}$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\theta$&lt;/td&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;$\theta(1-\theta)$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The Poisson distribution&lt;/strong&gt;
 &lt;script type=&quot;math/tex&quot;&gt;Poi(x|\lambda) = e^{-\lambda}\frac{\lambda^x}{x!}&lt;/script&gt;
 The first term is just the normalization constant, required to ensure the distribution sums to 1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The empirical distribution&lt;/strong&gt;	
 Given a set of data, $D = {x_1,\cdots,x_N}$, we define the &lt;strong&gt;empirical distribution&lt;/strong&gt;, also called the &lt;strong&gt;empirical measure&lt;/strong&gt;, as follows:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{emp}(A) \triangleq \frac{1}{N}\sum_{i=1}^N\delta_{x_i}(A)&lt;/script&gt;

    &lt;p&gt;ô°where $\delta_{x}(A)$ is the Dirac measure, defined by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\delta_{x}(A) = \begin{cases}
 0 &amp; \text{if } x \notin A\\
 1 &amp; \text{if } x \in A
 \end{cases} %]]&gt;&lt;/script&gt;

    &lt;p&gt;PS: in my opinion,  indicator function $\mathbb{I}_A(x)$ and Dirac measure $\delta_x(A)$ mean the same thing but their parameters are inversed.&lt;/p&gt;

    &lt;p&gt;In general, we can associate â€œweightsâ€ with each sample:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \sum_{i=1}^Nw_i\delta_{x_i}(x)&lt;/script&gt;

    &lt;p&gt;where it is required that $0\leq w_i\leq 1$ and $\sum w_i = 1$&lt;/p&gt;

    &lt;p&gt;According to Wiki,&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;In statistics, an empirical distribution function is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by $1/n$ at each of the $n$ data points.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;24-some-common-continuous-distributions&quot;&gt;2.4 Some common continuous distributions&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gaussian (normal) distribution&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|\mu, \theta) \triangleq \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}&lt;/script&gt;

    &lt;p&gt;Here $\sqrt{2\pi\sigma^2}$ is the normalization constant needed to ensure the density integrates to 1.(!Need to be proved)&lt;/p&gt;

    &lt;p&gt;Standard normal distribution is sometimes called the &lt;strong&gt;bell curve&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;We will often talk about the &lt;strong&gt;precision&lt;/strong&gt; of a Gaussian, by which we mean the inverse variance: $\lambda = \frac{1}{Ïƒ^2}$.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Question: Whatâ€™s the meaning of this sentence?&lt;/strong&gt;&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Note that, since this is a pdf, we can have $p(x) &amp;gt; 1$. To see this, consider evaluating the density at its center, $x = \mu$. We have $\mathcal{N}(\mu|\mu,\sigma^2) = (\sigma\sqrt{2\pi})^{âˆ’1}e^0$, so if $\sigma &amp;lt; 1/\sqrt{2\pi}$, we have $p(x) &amp;gt; 1$.
 Why does the author mention this?&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;The cumulative distribution function has no closed form expression but it can be calculated by &lt;strong&gt;error function(erf)&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Phi(x;\mu,\sigma)=\frac{1}{2}[1+erf(z/\sqrt{2})] \\ 
 erf(x) \triangleq \frac{2}{\sqrt{\pi}}\int_0^xe^{-t^2}dt&lt;/script&gt;

    &lt;p&gt;where $z=(x-\mu)/\sigma$.&lt;/p&gt;

    &lt;p&gt;Gaussian distribution is widely used because:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;it has two parameters which are easy to interpret&lt;/li&gt;
      &lt;li&gt;the central limit theorem (Section 2.6.3) tells us that sums of independent random variables have an approximately Gaussian distribution (good for modeling errors or noise)&lt;/li&gt;
      &lt;li&gt;Gaussian distribution makes the least number of assumptions &lt;strong&gt;(why?)&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;simple mathematical form&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Degenerate pdf&lt;/strong&gt;
 In the limit that $\sigma^2 \rightarrow 0$, the Gaussian becomes an infinitely tall and infinitely thin â€œspikeâ€ centered at $\mu$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{\sigma^2 \rightarrow 0} \mathcal{N}(x|\mu,\sigma^2)=\delta(x-\mu)&lt;/script&gt;

    &lt;p&gt;where $\delta$ is called a &lt;strong&gt;Dirac delta function&lt;/strong&gt;, defined as:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\delta(x) = \begin{cases}
 \infty &amp; \text{if } x=0\\
 0 &amp; \text{if }x\neq 0
 \end{cases} %]]&gt;&lt;/script&gt;

    &lt;p&gt;such that $\int_{-\infty}^{\infty}\delta(x)dx=1$&lt;/p&gt;

    &lt;p&gt;A useful property of delta functions is the &lt;strong&gt;sifting property&lt;/strong&gt;, which selects out a single term from a sum or integral:
 &lt;script type=&quot;math/tex&quot;&gt;\int_{-\infty}^{\infty}f(x)\delta(x-\mu)dx=f(\mu)&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;One problem with the Gaussian distribution is that it is sensitive to outliers (big change on $x$ results in bigger change on $e^{(x-\mu)^2}$)&lt;/p&gt;

    &lt;p&gt;Thus, A more robust distribution is the &lt;strong&gt;Student $t$ distribution&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{T}(x|\mu,\sigma^2,\nu) \propto \Big[1+\frac{1}{\nu}(\frac{x-\mu}{\sigma})^2\Big]^{-\frac{\nu+1}{2}}&lt;/script&gt;

    &lt;p&gt;where $\mu$ is the mean, $\sigma^2&amp;gt;0$ is the scale parameter and $\nu&amp;gt;0$ is the degrees of freedom. Its mean is $\mu$, mode is $\mu$,variance is $\frac{\nu\sigma^2}{(\nu-2)}$. The variance is only defined if $\nu &amp;gt; 2$. The mean is only defined if $\nu &amp;gt; 1$.&lt;/p&gt;

    &lt;p&gt;Because the Student has heavier tails, it hardly changes when adding some outliers. It is common to use $\nu = 4$ because of some good performance. But For $\nu \geq 5$, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The Laplace distribution&lt;/strong&gt; (a.k.a. double sided exponential distribution)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Lap(x|\mu,b) \triangleq \frac{1}{2b}exp(-\frac{|x-\mu|}{b})&lt;/script&gt;

    &lt;p&gt;Here $\mu$ is a location parameter and $b &amp;gt; 0$ is a scale parameter.&lt;/p&gt;

    &lt;p&gt;mean = $\mu$, mode = $\mu$, var = $2b^2$&lt;/p&gt;

    &lt;p&gt;It is also robust to outliers. It also put mores probability density at 0 than the Gaussian. This property is a useful way to encourage sparsity in a model (L1-regulation)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The gamma distribution&lt;/strong&gt;
 The gamma distribution is a flexible distribution for positive real valued rvâ€™s, $x &amp;gt; 0$. It is defined in terms of two parameters, called the shape $a &amp;gt; 0$ and the rate $b &amp;gt; 0$:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Ga(T|shape=a,rate=b) \triangleq \frac{b^a}{\Gamma(a)}T^{a-1}e^{-Tb}&lt;/script&gt;

    &lt;p&gt;where $\Gamma(a)$ is the gamma function:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Gamma(x) \triangleq \int_0^{\infty}u^{x-1}e^{-u}du&lt;/script&gt;

    &lt;p&gt;mean = $a/b$, mode = $(a-1/b)$, var = $a/b^2$&lt;/p&gt;

    &lt;p&gt;Gamma Distribution is also widely used, such as:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Exponential distribution: $Expon(x|\lambda) ô°—= Ga(x|1, \lambda)$&lt;/li&gt;
      &lt;li&gt;Erlang distribution: a is an integer&lt;/li&gt;
      &lt;li&gt;Chi-squared distribution: $\chi^2(x|\nu) ô°—= Ga(x|\nu,1)$&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Besides, if $X \sim Ga(a,b)$, then $1/X \sim IG(a,b)$, where IG is the inverse gamma distribution.&lt;/p&gt;

    &lt;p&gt;mean = $b/(a-1)$, mode = $b/(a+1)$, var = $b^2/((a-1)^2(a-2))$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The beta distribution&lt;/strong&gt;
 The beta distribution has support over the interval [0, 1] and is defined as follows:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Beta(x|a, b) = \frac{1}{B(a,b)} x^{aâˆ’1}(1 âˆ’ x)^{bâˆ’1}&lt;/script&gt;

    &lt;p&gt;where $B(a,b)$ is the beta function:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;B(a,b) \triangleq \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}&lt;/script&gt;

    &lt;p&gt;If $a = b = 1$, we get the uniform distirbution.&lt;/p&gt;

    &lt;p&gt;mean = $\frac{a}{b}$, mode = $\frac{a-1}{a+b-2}$, var = $\frac{ab}{(a+b)^2(a+b+1)}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pareto distribution
 The &lt;strong&gt;Pareto distribution&lt;/strong&gt; is used to model the distribution of quantities that exhibit long tails, also called &lt;strong&gt;heavy tails.&lt;/strong&gt; You may think of &lt;strong&gt;Pareto principle&lt;/strong&gt;, which is called â€œäºŒå…«åŸåˆ™â€ in Chinese.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Pareto(x|k, m) = km^kx^{âˆ’(k+1)}\mathbb{I}(x \geq m)&lt;/script&gt;

    &lt;p&gt;If we plot the distibution on a log-log scale, it form a straight line, of the form $\log p(x) = a\log x + c$ (a.k.a. power law)&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;You can know more &lt;strong&gt;power law&lt;/strong&gt; in &lt;a href=&quot;https://blog.csdn.net/jinruoyanxu/article/details/51627255&quot;&gt;æµ…è°ˆç½‘ç»œä¸–ç•Œä¸­çš„Power Lawç°è±¡ï¼ˆä¸€ï¼‰ ä»€ä¹ˆæ˜¯Power Law&lt;/a&gt;&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;mean = $\frac{km}{k-1}$ if $k&amp;gt;1$, mode = $m$, var = $\frac{m^2k}{(k-1)^2(k-2)}$ if $k &amp;gt; 2$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;25-joint-probability-distributions&quot;&gt;2.5 Joint probability distributions&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Covariance and correlation&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Uncorrelated does not imply independent&lt;/li&gt;
      &lt;li&gt;Independent does imply uncorrelated&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The multivariate Gaussian&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;multivariate Gaussian&lt;/strong&gt; or &lt;strong&gt;multivariate normal (MVN)&lt;/strong&gt; is the most widely used joint probability density function for continuous variables.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\pmb{x}|\pmb{\mu}, \pmb{\Sigma}) \triangleq \frac{1}{(2\pi)^{D/2}|\pmb{\Sigma}|^{1/2}}exp\Big[ -\frac{1}{2} (\pmb{x}-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\pmb{x}-\pmb{\mu}) \Big]&lt;/script&gt;

    &lt;p&gt;where $\mu = E[x] \in \mathbb{R}^D$ is the mean vector, and $\Sigma = cov[x]$ is the $D \times D$ covariance matrix. Sometimes we will work in terms of the &lt;strong&gt;precision matrix&lt;/strong&gt; or &lt;strong&gt;concentration matrix&lt;/strong&gt; instead. This is just the inverse covariance matrix, $\Lambda = \Sigma^{âˆ’1}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Multivariate Student $t$ distribution&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;A more robust alternative to the MVN is the multivariate Student t distribution. The smaller $\nu$ is, the fatter the tails.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dirichlet distribution&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;A multivariate generalization of the beta distribution is the &lt;strong&gt;Dirichlet distribution&lt;/strong&gt;, which has support over the &lt;strong&gt;probability simplex&lt;/strong&gt;, defined by&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;For example, a 2-simplex (or 2-probability-simplex) is a triangle.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_K=\{x:0\leq x_k\leq1, \sum_{k=1}^{K}x_k=1\}&lt;/script&gt;

    &lt;p&gt;The pdf is defined by:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;Dir(x|\alpha) \triangleq \frac{1}{B(\alpha)\prod_{k=1}^{K}x_k^{\alpha_k-1}}\mathbb{I}(x\in S_K)&lt;/script&gt;

    &lt;p&gt;where $B(\alpha) = \frac{\prod_{k=1}^{K}\Gamma(\alpha_k)}{\Gamma(\alpha_0)}$&lt;/p&gt;

    &lt;p&gt;We see that $\alpha_0 = \sum_k\alpha_k$ controls the strength of the distribution (how peaked it is), and the $\alpha_k$ control where the peak occurs. If $\alpha_k &amp;lt; 1$ for all k, we get â€œspikesâ€ at the corner of the simplex.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/in-post/2019-08-18-dir-ex.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;26-transformations-of-random-variables&quot;&gt;2.6 Transformations of random variables&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Linear transformations&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;linearity of expectation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;General transformations&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;If $X$ is a discrete rv, we can derive the pmf for $y$ by simply summing up the probability mass for all the $x$â€™s such that $f(x) = y$:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_y(y)=\sum_{x:f(x)=y}p_x(x)&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;If $X$ is continuous, we cannot use the above Equation since $p_x(x)$ is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdfâ€™s, and write&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P_y(y) \triangleq P(Y\leq y)=P(f(X) \leq y) = P(X\in {x|f(x)\leq y})&lt;/script&gt;

    &lt;p&gt;We can derive the pdf of y by differentiating the cdf.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_y(y)\triangleq\frac{d}{dy}P_y(y)=\frac{d}{dy}P_x(f^{-1}(y)) = \frac{dx}{dy}\frac{d}{dx}P_x(x)=\frac{dx}{dy}p_x(x)&lt;/script&gt;

    &lt;p&gt;where $x=f^{-1}(y)$. Since the sign of this change is not important, we take the absolute value to get the general expression:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_y(y)=\Big|\frac{dx}{dy}\Big|p_x(x)&lt;/script&gt;

    &lt;p&gt;This is called change of variables formula.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Central limit theorem&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;27-monte-carlo-approximation&quot;&gt;2.7 Monte Carlo approximation&lt;/h2&gt;

&lt;p&gt;In general, computing the distribution of a function of an rv using the change of variables formula can be difficult. One simple but powerful alternative is &lt;strong&gt;Monte Carlo approximation&lt;/strong&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First we generate $S$ samples from the distribution, call them $x_1,\cdots,x_S$. (There are many ways to generate such samples; one popular method, for high dimensional distributions, is called Markov chain Monte Carlo or MCMC; this will be explained later)&lt;/li&gt;
  &lt;li&gt;Given the samples, we can approximate the distribution of $f(X)$ by using the empirical distribution&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;28-information-theory&quot;&gt;2.8 Information theory&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Information theory&lt;/strong&gt; is concerned with representing data in a compact fashion (a task known as &lt;strong&gt;data compression&lt;/strong&gt; or &lt;strong&gt;source coding&lt;/strong&gt;), as well as with transmitting and storing it in a way that is robust to errors (a task known as &lt;strong&gt;error correction&lt;/strong&gt; or &lt;strong&gt;channel coding&lt;/strong&gt;).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Entropy&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The entropy of a random variable $X$ with distribution p, denoted by $\mathbb{H}(X)$ or sometimes $\mathbb{H}(p)$, is a measure of its &lt;strong&gt;uncertainty&lt;/strong&gt;. In particular, for a discrete variable with $K$ states, it is defined by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{H}(X) \triangleq -\sum_{k=1}^{K}p(X=k)log_2p(X=k)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;KL divergence&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;One way to measure the dissimilarity of two probability distributions, p and q, is known as the &lt;strong&gt;Kullback-Leibler divergence&lt;/strong&gt; (&lt;strong&gt;KL divergence&lt;/strong&gt;) or &lt;strong&gt;relative entropy&lt;/strong&gt;.&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{KL}(p||q) \triangleq \sum_{k=1}^{K}p_k\log\frac{p_k}{q_k} \\
 = \sum_k p_k \log p_k - \sum_k p_k \log q_k \\
 = - \mathbb{H}(p) + \mathbb{H}(p,q)&lt;/script&gt;

    &lt;p&gt;where $\mathbb{H}(p,q)$ is called the cross entropy, $\mathbb{H}(p,q) \triangleq -\sum_k p_k \log q_k$&lt;/p&gt;

    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$\bf{Theorem 2.8.1. :}$	(&lt;strong&gt;Information inequality&lt;/strong&gt;) $\mathbb{KL} (p&lt;/td&gt;
          &lt;td&gt;Â &lt;/td&gt;
          &lt;td&gt;q) \geq 0$ with equality iff $p = q$.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mutual information&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 18 Aug 2019 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2019/08/18/MLAPP-2Probability/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/18/MLAPP-2Probability/</guid>
        
        <category>Machine Learning</category>
        
        <category>Learning Notes</category>
        
        
      </item>
    
      <item>
        <title>Big Data Algorithm Lesson 1: About Kernel</title>
        <description>&lt;head&gt;
    &lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
    &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [ ['$$', '$$']],
            inlineMath: [['$','$']],
            processEscapes: true
            }
        });
    &lt;/script&gt;
&lt;/head&gt;

&lt;p&gt;æœ€è¿‘åœ¨æ—å¬å¤æ—¦å¤§å­¦ä¸€é—¨å¤§æ•°æ®ç®—æ³•è¯¾ï¼Œè€å¸ˆæ¯”è¾ƒä¾§é‡äºåº•å±‚ä¼˜åŒ–ç®—æ³•ï¼Œè®²çš„å¾ˆç»†è‡´å¾ˆæœ‰æ„æ€ï¼Œäºæ˜¯åœ¨çŸ¥ä¹ä¸Šæ•´ç†äº†ç¬”è®°ï¼Œç”±äºå…¬å¼å¤ªéš¾é‡æ–°æ‰“äº†ï¼Œé‚£ä¹ˆå°±æŠŠé“¾æ¥æ”¾åœ¨ä¸‹é¢å§ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45650399&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 1 - Why Kernel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/45848975&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 2 - Kernel K-means&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/48353576&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 3 - Kernel K-means extension&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/50768642&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 6/7-SupportVectorMachine Theorem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/51326678&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 8 - Optimal Condition &amp;amp; Dual SVM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52032287&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 9 - SVM &amp;amp; Algorithm (ADMM/ALM)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53177142&quot;&gt;ã€å¤§æ•°æ®ç®—æ³•è¯¾ç¨‹ç¬”è®°ã€‘Lesson 10- SVMï¼šProximal Gradient Method&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;why-kernel&quot;&gt;Why Kernel&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ä¸ºä»€ä¹ˆæ˜ å°„åˆ°é«˜ç»´ç©ºé—´å¯ä»¥è§£å†³çº¿æ€§ä¸å¯åˆ†çš„æƒ…å†µ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å¦‚ä¸‹å›¾ï¼Œè¿™ä¸€ç»„ç‚¹åœ¨äºŒç»´å¹³é¢çº¿æ€§ä¸å¯åˆ†ï¼Œä½†æ˜¯è‹¥å°†å…¶æ˜ å°„åˆ°ä¸‰ç»´ç©ºé—´ï¼Œå°±å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå¹³é¢å°†å…¶åˆ’åˆ†å¼€æ¥ã€‚ &lt;br /&gt;
é€šä¿—æ¥è¯´ï¼Œæ ¸å‡½æ•°çš„å°±æ˜¯å°†è¿™äº›ç‚¹ä»ä½ç»´ç©ºé—´æ˜ å°„åˆ°äº†é«˜ç»´ç©ºé—´ï¼Œä»¥è¾¾åˆ°çº¿æ€§å¯åˆ†çš„æ•ˆæœï¼Œä½†æ˜¯å…¶ç†è®ºåŸºç¡€æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ&lt;/p&gt;

&lt;p&gt;$\bf{Definition \ 1: }$
If ${ x_0,â€¦,x_N }, x_i \in R^{N+1}$ is affinity independent, they can be linearly seprable in n-dim space.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;what is affinity independent: &lt;br /&gt;
 If ${ (x_1-x_0),â€¦,(x_N-x_0) }$ is linearly independent, then ${ x_0,â€¦,x_N }$ is affinity independent.&lt;/li&gt;
  &lt;li&gt;what is linearly (strongly) seprable: &lt;br /&gt;
For two sets $C_1,C_2 \in R^N$,$\exists w \in R^N$, $s.t.inf_{x\in C_1}(w,x) &amp;gt; sup_{x\in C_2} (w,x)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;è¿™ä¸ªç†è®ºæä¾›äº†æ ¸å‡½æ•°å­˜åœ¨çš„æ„ä¹‰ï¼Œå› ä¸ºæ˜ å°„åˆ°é«˜ç»´ç©ºé—´åœ¨ç†è®ºä¸Šæ¥è¯´æ˜¯ä¸€ä¸ªå¯è¡Œçš„ã€èƒ½å¤Ÿæœ‰æ•ˆå¸®åŠ©è§£å†³çº¿æ€§ä¸å¯åˆ†æƒ…å†µçš„æ“ä½œã€‚å¦‚æœè¦é€šä¿—çš„æ¥ç†è§£è¿™ä¸ªç†è®ºï¼Œæ¯”å¦‚å¯¹äºä¸€ä¸ªdatasetï¼Œæœ‰500ä¸ªæ•°æ®æ ·æœ¬ï¼Œé‚£ä½ å¿…ç„¶èƒ½å¤Ÿæ‰¾ä¸€ä¸ª499ç»´çš„ç©ºé—´ï¼Œå°†è¿™500ä¸ªæ•°æ®ç‚¹åˆ†å¼€æ¥ï¼Œè€Œè¿™ä¸ª499ç»´çš„ç©ºé—´çš„åæ ‡è½´ï¼Œå®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬é€šå¸¸æ‰€è¯´çš„æŠ½å‡ºæ¥çš„ç‰¹å¾ã€‚&lt;/p&gt;

&lt;p&gt;ä¸ºäº†è¯æ˜è¿™ä¸ªç†è®ºï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä»¥ä¸‹å‡ ä¸ªçŸ¥è¯†ç‚¹ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;convex set: (å‡¸é›†)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a set $C$, if $\lambda x+(1-\lambda)y \in C, \ for \ \forall x,y\in C, \lambda \in (0,1) $&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;convex combination: (å‡¸ç»„åˆ)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\lambda_1x_1+\cdots+\lambda_nx_n$ is called a convex combination if $\lambda_i \ge 0$ and $\sum \lambda_i = 1$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;convex hull: (å‡¸åŒ…)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;conv(C): the convex hull of a set C is the intersection of all convex sets containing C.&lt;/p&gt;

&lt;p&gt;ä¸€ä¸ªé›†åˆçš„å‡¸åŒ…æŒ‡çš„å°±æ˜¯æ‰¾åˆ°çš„é‚£ä¸ªæœ€å°çš„é›†åˆä½¿å¾—å®ƒæˆä¸ºå‡¸é›†ï¼Œæˆ–è€…è¯´æ˜¯åŒ…å«è¿™ä¸ªé›†åˆçš„æœ€å°çš„å‡¸é›†ã€‚&lt;br /&gt;
conv(C) = {$x$|$x$ can be represented as the convex combination of points in $C$}&lt;/p&gt;

&lt;p&gt;å³ï¼Œå‡è®¾$C={x_0,â€¦,x_m}$, åˆ™å…¶å‡¸åŒ…$conv(C)={x|x=\sum \lambda_ix_i$, $\sum \lambda_i = 1$, $\lambda_i \ge 0} $&lt;/p&gt;

&lt;p&gt;æ³¨ï¼šæœ‰é™çš„ç‚¹ç»„æˆçš„convex hullä¸€å®šæ˜¯ä¸ªå¤šé¢ä½“ï¼Œä¸ä¼šæœ‰å¼§å½¢çš„éƒ¨åˆ†ï¼ˆå› ä¸ºè¦ä¿è¯æœ€å°ï¼‰ï¼Œè¿™ä¸ªå¤šé¢ä½“ä¹Ÿèƒ½è¢«ç§°ä¹‹ä¸ºä¸€ä¸ªsimplex-å•çº¯å½¢ã€‚&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Seperation Theorem: (å‡¸é›†åˆ†ç¦»å®šç†)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;å¯ä»¥è¯´æ˜¯çº¿æ€§è§„åˆ’æœ€ç»å…¸çš„ç†è®ºäº†&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å‡è®¾æœ‰ä¸¤ä¸ªconvex set $C_1,C_2$,$C_1\ne \emptyset $,$C_2\ne \emptyset$, ä¸”ä¸¤è€…é—­åŒ…ï¼ˆclosureï¼‰çš„äº¤é›†ç­‰äºç©ºï¼Œè¡¨ç¤ºä¸º$cl \ C_1 \cap cl \ C_2 = \emptyset$,è‹¥$C_1$$C_2$ä¸­ä»»æ„ä¸€ä¸ªé›†åˆæœ‰ç•Œ(bounded)ï¼Œåˆ™$\exists w\in R^N, \ s.t. \ inf_{x\in C_1}\langle w , x \rangle \ &amp;gt; \ sup_{x\in C_2}\langle w , x \rangle$&lt;/p&gt;

&lt;p&gt;äº†è§£äº†è¿™äº›ä¹‹åï¼Œæˆ‘ä»¬å°±èƒ½å¼€å§‹ç€æ‰‹è¯æ˜Definition1ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof Of Definition 1ï¼š&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å‡è®¾æœ‰ä¸¤ä¸ªé›†åˆ$S_1$å’Œ$S_2$ï¼Œ$S_1 \cup S_2\in {x_0,\cdots x_N}$, $S_1 \cap S_2 = \emptyset$ï¼ŒåŸºæœ¬æ€è·¯æ˜¯è¯æ˜è¿™ä¸¤ä¸ªé›†åˆçš„å‡¸åŒ…æ»¡è¶³å‡¸é›†åˆ†ç¦»å®šç†çš„ä½¿ç”¨æ¡ä»¶ï¼Œç„¶åå°±å¯ä»¥ä½¿ç”¨å‡¸é›†åˆ†ç¦»å®šç†ã€‚&lt;/p&gt;

&lt;p&gt;å–$C_1\doteq conv(S_1)$, $C_2\doteq conv(S_2)$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;è¯æ˜$C_1$, $C_2$æ˜¯convex set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;å‡¸åŒ…ä¸€å®šéƒ½æ˜¯å‡¸é›†ï¼Œæ‰€ä»¥è¿™ä¸ªæœ¬è´¨ä¸Šç­‰åŒäºè¯æ˜å‡¸åŒ…ä¸ºä»€ä¹ˆæ˜¯å‡¸é›†ï¼Œå‡¸åŒ…æ˜¯æŒ‡å¤šä¸ªå¤šä¸ªå‡¸é›†çš„äº¤é›†ï¼Œé‚£ä¹ˆè¿™ä¸ªè¯æ˜å°±è½¬åŒ–ä¸ºäº†è¯æ˜å¤šä¸ªå‡¸é›†çš„äº¤é›†è¿˜æ˜¯å‡¸é›†ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;&lt;br /&gt;
å‡è®¾æœ‰nä¸ªå‡¸é›†${C_1,\cdots,C_n}$ï¼Œå¯¹äºä»»æ„ä¸¤ä¸ªç‚¹$x_1$, $x_2$ï¼Œè‹¥å®ƒä»¬åœ¨è¿™nä¸ªå‡¸é›†çš„äº¤é›†ä¸­ï¼Œåˆ™å¯¹äºä»»æ„ä¸€ä¸ªå‡¸é›†$C_i$,éƒ½æ»¡è¶³$x_1,x_2\in C_i$ï¼Œåˆ™å¯¹äºæŸç‚¹$x_3$ï¼Œ$x_3=\lambda_ix_1+(1-\lambda_i)x_2\in C_i, \forall \ \lambda_i\in(0,1), \forall i$ï¼Œå¯¹ä»»æ„çš„$i$å’Œ$x_1$, $x_2$éƒ½æ»¡è¶³$x_3\in C_i$ï¼Œé‚£ä¹ˆ$x_3$è‡ªç„¶åœ¨
${C_1,\cdots,C_n}$çš„äº¤é›†ä¸­ï¼Œé‚£ä¹ˆèƒ½è¯æ˜${C_1,\cdots,C_n}$çš„äº¤é›†æ˜¯å‡¸é›†ã€‚&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;è¯æ˜$cl \ C_1 \cap cl \ C_2 = \emptyset$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;å…¶æ€è·¯å°±æ˜¯è¯æ˜$C_1 \cap C_2 = \emptyset$ä¸” $cl \ C_1=C_1$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$cl \ C_1=C_1$  ï¼Ÿ&lt;/li&gt;
  &lt;li&gt;å‡è®¾$\exists y \in C_1 \cap C_2$, åˆ™å¯å¾—å‡º&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$y=\sum_{i\in S_1}\alpha_ix_i=\sum_{j\in S_2}\beta_jx_j$, ä¸”$\sum_i\alpha_i=1, \sum_j\beta_j=1$,è”ç«‹æ–¹ç¨‹å¾—ï¼š&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left\{
\begin{aligned}
\sum_{i\in S_1}\alpha_ix_i - \sum_{j\in S_2}\beta_jx_j &amp; =  0 \\
\sum_i\alpha_i - \sum_j\beta_j &amp; =  0 \\
\end{aligned}
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;æ­¤æ–¹ç¨‹å¯ç®€åŒ–å†™ä¸ºï¼š&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left\{
\begin{aligned}
\sum_{i\in S_1\cup S_2}a_ix_i &amp; =  0 \\
\sum_{i\in S_1\cup S_2}a_i &amp; =  0 \\
\end{aligned}
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;å› æ­¤ï¼Œå¯¹ä»»æ„ä¸€ç»„${a_i}$,éƒ½è¦èƒ½æœ‰ä¸€ç»„${x_i}$è§£ï¼Œä½¿å¾—ä»¥ä¸Šç­‰å¼æˆç«‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†ä»¥ä¸Šç­‰å¼æ”¹å†™ä»¥ä¸‹å½¢å¼ï¼š&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} x_0 &amp; \cdots &amp; x_N \\ 1 &amp; \cdots &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} a_0\\ a_1 \\ \cdots \\ a_N\end{bmatrix}=0 %]]&gt;&lt;/script&gt;

&lt;p&gt;å¯ä»¥å‘ç°ï¼Œ$\begin{vmatrix} x_0 &amp;amp; \cdots &amp;amp; x_N \ 1 &amp;amp; \cdots &amp;amp; 1 \end{vmatrix}=\begin{vmatrix} x_0 &amp;amp; x_1-x_0 &amp;amp; \cdots &amp;amp; x_N-x_0 \ 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \end{vmatrix}$ï¼Œè€Œå­˜åœ¨æ¡ä»¶${ x_0,â€¦,x_N }, x_i \in R^{N+1}$è¿‘ä¼¼ç‹¬ç«‹ï¼Œå› æ­¤$\begin{bmatrix} x_1-x_0 &amp;amp; \cdots &amp;amp; x_N-x_0 \end{bmatrix}$çš„è¡Œåˆ—å¼ä¸ä¸ºé›¶ï¼Œå¯é€†ã€‚&lt;/p&gt;

&lt;p&gt;å› æ­¤å½“ä¸”ä»…å½“$\begin{bmatrix} a_0 &amp;amp; a_1 &amp;amp; \cdots &amp;amp; a_N\end{bmatrix}=0$æ—¶åŸå¼æ»¡è¶³ï¼Œæ‰€ä»¥ä¸å­˜åœ¨è¿™æ ·çš„ç‚¹$y$ï¼Œä¸æœ€åˆå‡è®¾çŸ›ç›¾ã€‚&lt;/p&gt;

&lt;p&gt;åˆ™$C_1 \cap C_2=\emptyset$è¯æ¯•&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ç»¼ä¸Šä¸¤ç‚¹ï¼Œå¯è¯æ˜$cl \ C_1 \cap cl \ C_2 = \emptyset$&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;å› æ­¤å¯¹äºä»»æ„çš„ä¸¤ä¸ªset $S_1$å’Œ$S_2$ï¼Œ$S_1 \cup S_2\in {x_0,\cdots x_N}$, $S_1 \cap S_2 = \emptyset$ï¼Œ$C_1 = conv(S_1)$å’Œ$C_2 = conv(S_2)$éƒ½æ»¡è¶³ä½¿ç”¨å‡¸é›†åˆ†ç¦»å®šç†çš„æ¡ä»¶ï¼Œå› æ­¤å¯¹ä»»æ„$S_1,S_2$éƒ½æœ‰ï¼š$\exists w, \ s.t. \ inf_{x\in C_1}\langle w , x \rangle \ &amp;gt; \ sup_{x\in C_2}\langle w , x \rangle$ï¼Œå› æ­¤${ x_0,â€¦,x_N }$åœ¨nç»´ç©ºé—´çº¿æ€§å¯åˆ†ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ï¼ˆåŒç†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è¯æ˜å‡ºï¼šn+2ä¸ªç‚¹åœ¨nç»´ä¸­çº¿æ€§ä¸å¯åˆ†ï¼‰&lt;/p&gt;

&lt;p&gt;æˆ‘ç†è§£æ¥è¯´ï¼Œè¿™ä¸ªç†è®ºå°±æ˜¯ä½¿ç”¨kernelè¿™ç§æ–¹æ³•çš„ç†è®ºæ”¯æ’‘ã€‚&lt;/p&gt;

&lt;h2 id=&quot;kernel--feature-map&quot;&gt;Kernel &amp;amp; Feature Map&lt;/h2&gt;

&lt;p&gt;è¿™ä¸ªåœ°æ–¹ä»…æä¾›æˆ‘è‡ªå·±çš„ç†è§£ï¼Œä¾‹å¦‚å¯¹äºé«˜æ–¯æ ¸ï¼Œ$G(x_i, x_j)=\exp(-||x_i-x_j||^2)$ï¼Œæˆ‘ä»¬å¹³æ—¶ä¼šç›´æ¥ä½¿ç”¨è¿™æ ·çš„å˜æ¢ï¼Œä½†å´ä¸çŸ¥é“è¿™æ ·å˜æ¢çš„åŸå› æ‰€åœ¨ï¼Œè€Œå…¶å®ä»$(x_i,x_j)$åˆ°$G(x_i, x_j)$ä¹‹é—´è¿˜æœ‰è¿™æ ·ä¸€ä¸ªæµç¨‹ï¼š&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_i,x_j) \rightarrow K(x_i,x_j) = \langle \phi(x_i), \phi(x_j) \rangle_H\rightarrow G(x_i, x_j)\\&lt;/script&gt;

&lt;p&gt;å› ä¸º$(x_i,x_j)$å¯ä»¥è¢«ä¸€ä¸ªæ˜ å°„å‡½æ•°$\phi(\cdot)$æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´ç©ºé—´ï¼Œåœ¨è¿™ä¸ªå¸Œå°”ä¼¯ç‰¹ç©ºé—´æœ‰ä¸€ç§å†…ç§¯çš„æ“ä½œï¼Œå†…ç§¯å¯ä»¥è¡¨ç¤º$\phi(x_i),\phi(x_j)$ä¸¤è€…çš„ç›¸ä¼¼æ€§ï¼Œä¸”è¿™ä¸ªå€¼åˆšå¥½ç­‰äºä½ç»´ç©ºé—´ä¸‹$G(x_i, x_j)$çš„å€¼ï¼Œå› æ­¤ï¼Œè¿™æ ·å»åšä¸€ä¸ªkernelæ“ä½œæ˜¯åˆç†çš„ã€‚å…¶ä¸­ï¼Œè¿™ä¸€æ•´å¥—è¿‡ç¨‹è¢«ç§°ä¸ºkernel methodï¼Œè€Œæ˜ å°„çš„è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä½œFeature Mapã€‚ï¼ˆå› ä¸ºæˆ‘ä¹‹å‰ä¸€ç›´æœ‰ä¸€ä¸ªè¯¯åŒºï¼Œä»¥ä¸ºkernelæŒ‡çš„å°±æ˜¯ä»ä½ç»´ç©ºé—´æ˜ å°„åˆ°é«˜ç»´ç©ºé—´çš„è¿™ä¹ˆä¸€ä¸ªæ“ä½œï¼Œå…¶å®ä¸ç„¶ï¼Œè¿™ä¸ªè¿‡ç¨‹åªæ˜¯kernel methodçš„ä¸€éƒ¨åˆ†è€Œå·²ï¼‰&lt;/p&gt;

&lt;p&gt;$\bf{Mercerâ€™s \ theorem :}$
Mercerâ€™s theorem is a representation of a symmetric positive-definite function on a square as a sum of a convergent sequence of product functions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å¯¹æ¯”è€å¸ˆåœ¨ä¸Šè¯¾æ—¶ç»™å‡ºçš„å®šä¹‰ä¸ºï¼š&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If $G = G^T$ and $G \geq 0$, $\exists \phi(\cdot), \ s.t. \ G_{ij} = K(x_i,x_j) = \langle \phi(x_i) , \phi(x_j) \rangle_H$&lt;/p&gt;

&lt;p&gt;å…¶å®è€å¸ˆä¸Šè¯¾è®²åˆ°çš„è¿™ä¸ªå®šä¹‰å°±æ˜¯mercerå®šç†äº†ï¼Œå³åªè¦Gæ»¡è¶³è¿™ä¸¤ä¸ªæ¡ä»¶ï¼Œé‚£ä¹ˆå®ƒå°±èƒ½æ‰¾åˆ°è¿™æ ·çš„æ˜ å°„å‡½æ•°ä»¥åŠä¸€ç§å†…ç§¯æ–¹å¼ï¼Œé‚£ä¹ˆå°±æ˜¯åˆç†çš„æ ¸äº†ã€‚&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Referenceï¼š&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.zhihu.com/question/24627666&lt;/li&gt;
  &lt;li&gt;https://mp.weixin.qq.com/s?__biz=MzIxNDIwMTk2OQ==&amp;amp;mid=2649077019&amp;amp;idx=1&amp;amp;sn=e0c4a6c502e3668e1dc410f21e531cfd&amp;amp;scene=0#wechat_redirect&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/wsj998689aa/article/details/47027365&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/wsj998689aa/article/details/40398777&lt;/li&gt;
  &lt;li&gt;https://www.cnblogs.com/xingshansi/p/6767980.html&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/zhazhiqiang/article/details/19496633&lt;/li&gt;
  &lt;li&gt;https://blog.csdn.net/cqy_chen/article/details/77932270&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 30 Sep 2018 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2018/09/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%97%E6%B3%95-lesson1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/09/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%97%E6%B3%95-lesson1/</guid>
        
        <category>Algirithms</category>
        
        <category>Lecture Notes</category>
        
        
      </item>
    
  </channel>
</rss>
