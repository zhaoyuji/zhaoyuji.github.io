---
layout:     post
title:      "Machin Learning Models Summary & Implement"
subtitle:   ""
author:     "Z"
header-style: text
catalog: true
tags:
    - Algirithms
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [ ['$$', '$$']],
            inlineMath: [['$','$']],
            processEscapes: true
            }
        });
    </script>
</head>





## SVM

SVM详细的原理已经在去年整理在：

1. [【大数据算法课程笔记】Lesson 6/7-SupportVectorMachine Theorem](https://zhuanlan.zhihu.com/p/50768642)  
2. [【大数据算法课程笔记】Lesson 8 - Optimal Condition & Dual SVM](https://zhuanlan.zhihu.com/p/51326678)  
3. [【大数据算法课程笔记】Lesson 9 - SVM & Algorithm (ADMM/ALM)](https://zhuanlan.zhihu.com/p/52032287)   
4. [【大数据算法课程笔记】Lesson 10- SVM：Proximal Gradient Method](https://zhuanlan.zhihu.com/p/53177142)  


**Code:**  
[SVM - Python Code & Examples](https://github.com/zhaoyuji/KTH-Machine-Learning-Course-Lab-DD2421/blob/master/Lab2%20SVM/pysvm.py) 



## KNN

**算法流程：**　　

1. 计算测试数据与各个训练数据之间的距离；　
2. 按照距离的递增关系进行排序；　　
3. 选取距离最小的K个点；　　
4. 确定前K个点所在类别的出现频率；　　
5. 返回前K个点中出现频率最高的类别作为测试数据的预测分类　　

Reference: https://www.cnblogs.com/jyroy/p/9427977.html

**Code:**  
soon...


**Pros:**

1. 简单粗暴，无需训练
2. 适合对稀有事件进行分类
3. 特别适合于多分类问题，表现比SVM要好

**Cons:**

1. 对待样本不平衡的数据集效果较差
2. 计算量大
3. 可理解性差


## Naive Bayes

soon...


**Code:** 

连续特征 - [Bayes Classifier and Boosting - Python Code & Examples](https://github.com/zhaoyuji/KTH-Machine-Learning-Course-Lab-DD2421/blob/master/Lab3%20Bayes%20boosting/lab3.ipynb) 

离散特征 - soon...






